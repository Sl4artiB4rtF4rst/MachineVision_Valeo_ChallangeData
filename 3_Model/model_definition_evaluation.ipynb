{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:05.753948Z",
     "start_time": "2025-12-15T09:47:05.749892Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.src.legacy.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from matplotlib.ticker import StrMethodFormatter\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We are considering and evaluating self-defined CNN and pretrained CNNs. Convolutional Neural Networks are proven to work well for the task of image classification. Most likely transformer models could potentially have an even better performance but it is highly likely that the added complexity will not justify the potential increase in performance.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameters\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:05.771478Z",
     "start_time": "2025-12-15T09:47:05.766953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# choose main hyperparameters here\n",
    "\n",
    "#data / feature selections\n",
    "balanced_flag = True\n",
    "\n",
    "#Traing data splits :\n",
    "test_split = 0.2\n",
    "val_split = 0.20 # remember - this is fractional  after the test data has been split from the initial balanced sub dataset\n",
    "\n",
    "# image parameters\n",
    "target_size = (299,299) #pixel size to load img\n",
    "batch_size = 8 #later player around with batch size to see how it affects performance\n",
    "\n",
    "#select data augmentation\n",
    "aug_flag = False\n",
    "\n",
    "#training\n",
    "max_epochs = 100\n",
    "loss_stop_patience = 7"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "[Describe any additional feature engineering you've performed beyond what was done for the baseline model.]\n",
    "\n",
    "We test data augmentation (to varying degrees), dataset balancing and class weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:05.792356Z",
     "start_time": "2025-12-15T09:47:05.777481Z"
    }
   },
   "source": [
    "# Path Definitions to relevant data + data loading\n",
    "\n",
    "base_file_path = 'C:/Users/nikoLocal/Documents/Opencampus/Machine_Vision_challenge_data/'\n",
    "image_path = base_file_path + '/input_train/input_train'\n",
    "\n",
    "label_csv_name = 'Y_train_eVW9jym.csv'\n",
    "\n",
    "#Loading .csv data to dataframes\n",
    "train_df = pd.read_csv(os.path.join(base_file_path, label_csv_name))\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:05.823258Z",
     "start_time": "2025-12-15T09:47:05.802361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#DataFrame Preprocessing\n",
    "\n",
    "\n",
    "#add another column to the dataframe according to dictionaries to map Labels correctly to numbers\n",
    "dict_numbers = {'GOOD': 0,'Boucle plate':1,'Lift-off blanc':2,'Lift-off noir':3,'Missing':4,'Short circuit MOS':5}\n",
    "dict_strings = {'GOOD': '0_GOOD','Boucle plate':'1_Flat loop','Lift-off blanc':'2_White lift-off','Lift-off noir':'3_Black lift-off','Missing':'4_Missing','Short circuit MOS':'5_Short circuit MOS'}\n",
    "# for Test Data (\"random submission\" dataframe)\n",
    "dict_strings_sub = {0: '0_GOOD',1:'1_Flat loop',2:'2_White lift-off',3:'3_Black lift-off',4:'4_Missing',5:'5_Short circuit MOS',6:'6_Drift'}\n",
    "\n",
    "#list of all labels in the data\n",
    "label_list = ['0_GOOD','1_Flat loop','2_White lift-off','3_Black lift-off','4_Missing','5_Short circuit MOS']\n",
    "\n",
    "#create new columns in DFs via .map() method\n",
    "train_df['LabelNum'] = train_df['Label'].map(dict_numbers)\n",
    "train_df['LabelStr'] = train_df['Label'].map(dict_strings)\n",
    "\n",
    "#number of classes\n",
    "num_classes = len(label_list)\n",
    "\n",
    "# get counts of label with the least entries\n",
    "countList = train_df['LabelStr'].value_counts()\n",
    "minCounts = countList.min()\n",
    "\n",
    "BalancedDF = pd.DataFrame()\n",
    "#concat sampled dataframes for each included label\n",
    "for i in range(num_classes):\n",
    "    BalancedDF = pd.concat([BalancedDF,train_df[train_df['LabelStr'] == label_list[i]].sample(n=minCounts)],axis=0)\n",
    "\n",
    "#test if worked as intended\n",
    "print(BalancedDF['LabelStr'].value_counts())\n",
    "\n",
    "#split dataframe according to fractional test size\n",
    "train_df_balanced, test_df_balanced = train_test_split(BalancedDF, test_size=test_split, random_state=42) #keep random state constant to ensure\n",
    "\n",
    "train_df_train, train_df_test = train_test_split(train_df, test_size=test_split, random_state=42) #keep random state constant to ensure"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelStr\n",
      "0_GOOD                 71\n",
      "1_Flat loop            71\n",
      "2_White lift-off       71\n",
      "3_Black lift-off       71\n",
      "4_Missing              71\n",
      "5_Short circuit MOS    71\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:07.587246Z",
     "start_time": "2025-12-15T09:47:05.834262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialize ImageDataGenerators\n",
    "# use ImageDataGen because it has method flow_from_dataframe() that works really well together with pandas dataframes\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# although deprecated the functionality can be used as discussed in feedback session\n",
    "\n",
    "# HYPERPARAMTERS ########\n",
    "\n",
    "\n",
    "class_mode = 'categorical' # how to store labels - either categorical (one-hot encoding) or as numbers\n",
    "#class_mode = 'input'\n",
    "labelCol = 'LabelStr'\n",
    "#########################\n",
    "\n",
    "#normalize pixel intensities\n",
    "rescale = 1.0/255.0\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=0.0,\n",
    "    shear_range=0.0,\n",
    "    rescale=rescale,\n",
    "    validation_split=val_split)\n",
    "\n",
    "datagen_augmentation = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rotation_range=10,\n",
    "    shear_range= 5,\n",
    "    zoom_range = 0.05,\n",
    "    rescale=rescale,\n",
    "    validation_split=val_split)\n",
    "\n",
    "datagen_test = ImageDataGenerator(\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=0.0,\n",
    "    shear_range=0.0,\n",
    "    rescale=rescale,\n",
    "    validation_split=0.0)\n",
    "\n",
    "\n",
    "##########################################################\n",
    "\n",
    "#unbalanced datasets\n",
    "\n",
    "train_generator_unbalanced = datagen.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_unbalanced_val = datagen.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "train_generator_unbalanced_aug = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_unbalanced_val_aug = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "test_generator_unbalanced = datagen_test.flow_from_dataframe(\n",
    "    train_df_test,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "test_generator_unbalanced_metrics = datagen_test.flow_from_dataframe(\n",
    "    train_df_test,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_val = datagen.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "train_generator_aug = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_aug_val = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "test_generator = datagen_test.flow_from_dataframe(\n",
    "    test_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "test_generator_metrics = datagen_test.flow_from_dataframe(\n",
    "    test_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "# generators for transfer learning - color mode is color here. Pretrained models expect color input\n",
    "\n",
    "train_generator_unbalanced_color = datagen.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_unbalanced_val_color = datagen.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "test_generator_unbalanced_color = datagen_test.flow_from_dataframe(\n",
    "    train_df_test,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "test_generator_unbalanced_metrics_color = datagen_test.flow_from_dataframe(\n",
    "    train_df_test,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_color = datagen.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_val_color = datagen.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "train_generator_aug_color = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_aug_val_color = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "test_generator_color = datagen_test.flow_from_dataframe(\n",
    "    test_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "test_generator_metrics_color = datagen_test.flow_from_dataframe(\n",
    "    test_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5298 validated image filenames belonging to 6 classes.\n",
      "Found 1324 validated image filenames belonging to 6 classes.\n",
      "Found 5298 validated image filenames belonging to 6 classes.\n",
      "Found 1324 validated image filenames belonging to 6 classes.\n",
      "Found 1656 validated image filenames belonging to 6 classes.\n",
      "Found 1656 validated image filenames belonging to 6 classes.\n",
      "Found 272 validated image filenames belonging to 6 classes.\n",
      "Found 68 validated image filenames belonging to 6 classes.\n",
      "Found 272 validated image filenames belonging to 6 classes.\n",
      "Found 68 validated image filenames belonging to 6 classes.\n",
      "Found 86 validated image filenames belonging to 6 classes.\n",
      "Found 86 validated image filenames belonging to 6 classes.\n",
      "Found 5298 validated image filenames belonging to 6 classes.\n",
      "Found 1324 validated image filenames belonging to 6 classes.\n",
      "Found 1656 validated image filenames belonging to 6 classes.\n",
      "Found 1656 validated image filenames belonging to 6 classes.\n",
      "Found 272 validated image filenames belonging to 6 classes.\n",
      "Found 68 validated image filenames belonging to 6 classes.\n",
      "Found 272 validated image filenames belonging to 6 classes.\n",
      "Found 68 validated image filenames belonging to 6 classes.\n",
      "Found 86 validated image filenames belonging to 6 classes.\n",
      "Found 86 validated image filenames belonging to 6 classes.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "[Discuss any hyperparameter tuning methods you've applied, such as Grid Search or Random Search, and the rationale behind them.]\n",
    "So far we have done hyperparameters variation \"by hand\" only. Some parameters, such as the image size and data augmentation have been systemarically varied and the effects on model performance noted.\n",
    "\n",
    "We plan to do more hyperparamter tuning in the future."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:07.597712Z",
     "start_time": "2025-12-15T09:47:07.592249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# make a unique string (name) to save model and evaluation to file\n",
    "# incorporate most important hyperparameters\n",
    "# make a subfolder for one set of hyperparameters for more tidy folder and file structure\n",
    "\n",
    "if aug_flag:\n",
    "    augmentation_str = 'Aug'\n",
    "else:\n",
    "    augmentation_str = 'NoAug'\n",
    "\n",
    "if balanced_flag:\n",
    "    balance_str = 'balanced'\n",
    "else:\n",
    "    balance_str = 'unbalanced'\n",
    "\n",
    "hyperparam_name = 'ImgSz_{}_{}_{}'.format(target_size[0],augmentation_str,balance_str)\n",
    "hyperparam_dir = os.path.join(base_file_path,'model_evaluation')\n",
    "hyperparam_dir = os.path.join(hyperparam_dir,hyperparam_name)\n",
    "#check if folder exists - if not create it\n",
    "if not os.path.isdir(hyperparam_dir):\n",
    "    os.makedirs(hyperparam_dir)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement the final model(s) you've selected based on the above steps.]\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:08.252866Z",
     "start_time": "2025-12-15T09:47:07.607717Z"
    }
   },
   "source": [
    "# build a model to be used as baseline model\n",
    "# use \"simplest\" CNN as baseline\n",
    "\n",
    "model_1_CNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input((target_size[0], target_size[1], 1)),  #image are greyscale - so in total dim (width,height,1)\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model_2_CNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input((target_size[0], target_size[1], 1)),  #image are greyscale - so in total dim (width,height,1)\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model_3_CNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input((target_size[0], target_size[1], 1)),  #image are greyscale - so in total dim (width,height,1)\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model_1_CNN.summary()\n",
    "model_2_CNN.summary()\n",
    "model_3_CNN.summary()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001B[38;5;33mConv2D\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m297\u001B[0m, \u001B[38;5;34m297\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │           \u001B[38;5;34m640\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001B[38;5;33mMaxPooling2D\u001B[0m)    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m148\u001B[0m, \u001B[38;5;34m148\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001B[38;5;33mFlatten\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1401856\u001B[0m)        │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │    \u001B[38;5;34m89,718,848\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │           \u001B[38;5;34m390\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1401856</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │    <span style=\"color: #00af00; text-decoration-color: #00af00\">89,718,848</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m89,719,878\u001B[0m (342.25 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,719,878</span> (342.25 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m89,719,878\u001B[0m (342.25 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,719,878</span> (342.25 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m297\u001B[0m, \u001B[38;5;34m297\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │           \u001B[38;5;34m320\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m148\u001B[0m, \u001B[38;5;34m148\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m146\u001B[0m, \u001B[38;5;34m146\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │        \u001B[38;5;34m18,496\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m73\u001B[0m, \u001B[38;5;34m73\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001B[38;5;33mFlatten\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m341056\u001B[0m)         │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │    \u001B[38;5;34m43,655,296\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │           \u001B[38;5;34m774\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">341056</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">43,655,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m43,674,886\u001B[0m (166.61 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,674,886</span> (166.61 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m43,674,886\u001B[0m (166.61 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,674,886</span> (166.61 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_2\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m297\u001B[0m, \u001B[38;5;34m297\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │           \u001B[38;5;34m320\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m148\u001B[0m, \u001B[38;5;34m148\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m146\u001B[0m, \u001B[38;5;34m146\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │        \u001B[38;5;34m18,496\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m73\u001B[0m, \u001B[38;5;34m73\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m71\u001B[0m, \u001B[38;5;34m71\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │        \u001B[38;5;34m73,856\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m35\u001B[0m, \u001B[38;5;34m35\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001B[38;5;33mFlatten\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m156800\u001B[0m)         │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │    \u001B[38;5;34m20,070,528\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m8,256\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │           \u001B[38;5;34m390\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156800</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,070,528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m20,171,846\u001B[0m (76.95 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,171,846</span> (76.95 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m20,171,846\u001B[0m (76.95 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,171,846</span> (76.95 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:10.295127Z",
     "start_time": "2025-12-15T09:47:08.573623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Transfer learning model - feature extraction\n",
    "\n",
    "# Load pre-trained InceptionV3 with correct input size\n",
    "base_transfer_model = tf.keras.applications.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(target_size[0], target_size[0], 3)\n",
    ")\n",
    "\n",
    "# Freeze all layers for feature extraction\n",
    "base_transfer_model.trainable = False\n",
    "\n",
    "# Simple classification head\n",
    "# - GlobalAveragePooling2D reduces spatial dimensions\n",
    "# - Final Dense layer maps to class probabilities\n",
    "inception_feature_extraction_model = tf.keras.Sequential([\n",
    "    base_transfer_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "inception_feature_extraction_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:10.314271Z",
     "start_time": "2025-12-15T09:47:10.301130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compile Models\n",
    "\n",
    "model_1_CNN.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "model_2_CNN.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "model_3_CNN.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "#F1 average parameter needs to be anything other than None if using linewise output when fiting the model...\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:10.322927Z",
     "start_time": "2025-12-15T09:47:10.319268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# callback that monitors validation accuracy / loss\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "val_loss_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.01,\n",
    "    patience= loss_stop_patience,\n",
    "    restore_best_weights=True,\n",
    "    verbose = 2,\n",
    "    start_from_epoch = 5\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:47:10.332116Z",
     "start_time": "2025-12-15T09:47:10.327923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#select model datasets based on flags\n",
    "\n",
    "#for model1 - 3\n",
    "if balanced_flag:\n",
    "    CNN_model_val_gen = train_generator_val\n",
    "    if aug_flag:\n",
    "        CNN_model_gen = train_generator_aug\n",
    "    else:\n",
    "        CNN_model_gen = train_generator\n",
    "else:\n",
    "    CNN_model_val_gen = train_generator_unbalanced_val\n",
    "    if aug_flag:\n",
    "        CNN_model_gen = train_generator_unbalanced_aug\n",
    "    else:\n",
    "        CNN_model_gen = train_generator_unbalanced\n",
    "\n",
    "#for transfer learning\n",
    "if balanced_flag:\n",
    "    transfer_model_val_gen = train_generator_val_color\n",
    "    if aug_flag:\n",
    "        transfer_model_gen = train_generator_aug_color\n",
    "    else:\n",
    "        transfer_model_gen = train_generator_color\n",
    "else:\n",
    "    transfer_model_val_gen = train_generator_unbalanced_val_color\n",
    "    if aug_flag:\n",
    "        transfer_model_gen = train_generator_unbalanced_aug_color\n",
    "    else:\n",
    "        transfer_model_gen = train_generator_unbalanced_color"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-15T09:47:10.338119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Model 1\n",
    "history_1 = model_1_CNN.fit(\n",
    "CNN_model_gen,\n",
    "validation_data = CNN_model_val_gen,\n",
    "epochs=max_epochs,\n",
    "callbacks=[val_loss_stop],\n",
    "verbose = 2 #2 is one line per epoch -\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 - 26s - 767ms/step - accuracy: 0.3566 - f1_score: 0.3563 - loss: 10.7637 - precision: 0.3796 - recall: 0.3419 - val_accuracy: 0.7206 - val_f1_score: 0.7163 - val_loss: 1.6770 - val_precision: 0.7164 - val_recall: 0.7059\n",
      "Epoch 2/100\n",
      "34/34 - 24s - 698ms/step - accuracy: 0.8199 - f1_score: 0.8179 - loss: 0.6644 - precision: 0.8514 - recall: 0.7794 - val_accuracy: 0.9118 - val_f1_score: 0.9121 - val_loss: 0.4398 - val_precision: 0.9375 - val_recall: 0.8824\n",
      "Epoch 3/100\n",
      "34/34 - 24s - 693ms/step - accuracy: 0.9779 - f1_score: 0.9778 - loss: 0.0961 - precision: 0.9925 - recall: 0.9706 - val_accuracy: 0.8971 - val_f1_score: 0.8978 - val_loss: 0.4340 - val_precision: 0.8971 - val_recall: 0.8971\n",
      "Epoch 4/100\n",
      "34/34 - 23s - 689ms/step - accuracy: 0.9963 - f1_score: 0.9963 - loss: 0.0292 - precision: 0.9963 - recall: 0.9963 - val_accuracy: 0.8676 - val_f1_score: 0.8723 - val_loss: 0.5441 - val_precision: 0.8788 - val_recall: 0.8529\n",
      "Epoch 5/100\n",
      "34/34 - 23s - 691ms/step - accuracy: 0.9926 - f1_score: 0.9926 - loss: 0.0326 - precision: 0.9926 - recall: 0.9926 - val_accuracy: 0.8824 - val_f1_score: 0.8835 - val_loss: 0.3450 - val_precision: 0.8806 - val_recall: 0.8676\n",
      "Epoch 6/100\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:34:29.462196Z",
     "start_time": "2025-12-14T22:34:14.055003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Model 2\n",
    "\n",
    "history_2 = model_2_CNN.fit(\n",
    "    CNN_model_gen,\n",
    "    validation_data = CNN_model_val_gen,\n",
    "    epochs=max_epochs,\n",
    "    callbacks=[val_loss_stop],\n",
    "    verbose = 2 #2 is one line per epoch -\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 - 12s - 351ms/step - accuracy: 0.4890 - f1_score: 0.4774 - loss: 1.3807 - precision: 0.8182 - recall: 0.1324 - val_accuracy: 0.5882 - val_f1_score: 0.5599 - val_loss: 1.2022 - val_precision: 0.7879 - val_recall: 0.3824\n",
      "Epoch 2/100\n",
      "34/34 - 12s - 340ms/step - accuracy: 0.6507 - f1_score: 0.6505 - loss: 0.8945 - precision: 0.7819 - recall: 0.5404 - val_accuracy: 0.6324 - val_f1_score: 0.6436 - val_loss: 1.0595 - val_precision: 0.7193 - val_recall: 0.6029\n",
      "Epoch 3/100\n",
      "34/34 - 11s - 331ms/step - accuracy: 0.7941 - f1_score: 0.7935 - loss: 0.5762 - precision: 0.8299 - recall: 0.7353 - val_accuracy: 0.7941 - val_f1_score: 0.7994 - val_loss: 0.7928 - val_precision: 0.8667 - val_recall: 0.7647\n",
      "Epoch 4/100\n",
      "34/34 - 11s - 336ms/step - accuracy: 0.8860 - f1_score: 0.8846 - loss: 0.3684 - precision: 0.9186 - recall: 0.8713 - val_accuracy: 0.7794 - val_f1_score: 0.7846 - val_loss: 0.6861 - val_precision: 0.8030 - val_recall: 0.7794\n",
      "Epoch 5/100\n",
      "34/34 - 12s - 364ms/step - accuracy: 0.8860 - f1_score: 0.8857 - loss: 0.2960 - precision: 0.9008 - recall: 0.8676 - val_accuracy: 0.8382 - val_f1_score: 0.8328 - val_loss: 0.4914 - val_precision: 0.8594 - val_recall: 0.8088\n",
      "Epoch 6/100\n",
      "34/34 - 11s - 332ms/step - accuracy: 0.8603 - f1_score: 0.8587 - loss: 0.4241 - precision: 0.8736 - recall: 0.8382 - val_accuracy: 0.7941 - val_f1_score: 0.7980 - val_loss: 0.6632 - val_precision: 0.8030 - val_recall: 0.7794\n",
      "Epoch 7/100\n",
      "34/34 - 12s - 342ms/step - accuracy: 0.8934 - f1_score: 0.8927 - loss: 0.3382 - precision: 0.9087 - recall: 0.8787 - val_accuracy: 0.8235 - val_f1_score: 0.8264 - val_loss: 0.3923 - val_precision: 0.8710 - val_recall: 0.7941\n",
      "Epoch 8/100\n",
      "34/34 - 11s - 327ms/step - accuracy: 0.9191 - f1_score: 0.9185 - loss: 0.2616 - precision: 0.9318 - recall: 0.9044 - val_accuracy: 0.8235 - val_f1_score: 0.8286 - val_loss: 0.7685 - val_precision: 0.8235 - val_recall: 0.8235\n",
      "Epoch 9/100\n",
      "34/34 - 10s - 308ms/step - accuracy: 0.9007 - f1_score: 0.8982 - loss: 0.2637 - precision: 0.9071 - recall: 0.8971 - val_accuracy: 0.8088 - val_f1_score: 0.8176 - val_loss: 0.6128 - val_precision: 0.8209 - val_recall: 0.8088\n",
      "Epoch 10/100\n",
      "34/34 - 10s - 296ms/step - accuracy: 0.9007 - f1_score: 0.9001 - loss: 0.2701 - precision: 0.9094 - recall: 0.8860 - val_accuracy: 0.8235 - val_f1_score: 0.8325 - val_loss: 0.6880 - val_precision: 0.8358 - val_recall: 0.8235\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n"
     ]
    }
   ],
   "execution_count": 325
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:34:29.462196Z",
     "start_time": "2025-12-14T22:32:06.334267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Model 3\n",
    "\n",
    "\n",
    "history_3 = model_3_CNN.fit(\n",
    "    CNN_model_gen,\n",
    "    validation_data = CNN_model_val_gen,\n",
    "    epochs=max_epochs,\n",
    "    callbacks=[val_loss_stop],\n",
    "    verbose = 2 #2 is one line per epoch -\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 - 9s - 257ms/step - accuracy: 0.4926 - f1_score: 0.4724 - loss: 1.3302 - precision: 0.7458 - recall: 0.1618 - val_accuracy: 0.4706 - val_f1_score: 0.4205 - val_loss: 1.4910 - val_precision: 0.5909 - val_recall: 0.3824\n",
      "Epoch 2/100\n",
      "34/34 - 9s - 255ms/step - accuracy: 0.6838 - f1_score: 0.6807 - loss: 0.7958 - precision: 0.8043 - recall: 0.5441 - val_accuracy: 0.6471 - val_f1_score: 0.6135 - val_loss: 0.9469 - val_precision: 0.6825 - val_recall: 0.6324\n",
      "Epoch 3/100\n",
      "34/34 - 9s - 257ms/step - accuracy: 0.7978 - f1_score: 0.7958 - loss: 0.5722 - precision: 0.8408 - recall: 0.7574 - val_accuracy: 0.7206 - val_f1_score: 0.7208 - val_loss: 0.6413 - val_precision: 0.7500 - val_recall: 0.7059\n",
      "Epoch 4/100\n",
      "34/34 - 8s - 249ms/step - accuracy: 0.8493 - f1_score: 0.8494 - loss: 0.4615 - precision: 0.8798 - recall: 0.8346 - val_accuracy: 0.8382 - val_f1_score: 0.8391 - val_loss: 0.6371 - val_precision: 0.8636 - val_recall: 0.8382\n",
      "Epoch 5/100\n",
      "34/34 - 9s - 260ms/step - accuracy: 0.8824 - f1_score: 0.8809 - loss: 0.3718 - precision: 0.8931 - recall: 0.8603 - val_accuracy: 0.8382 - val_f1_score: 0.8388 - val_loss: 0.6651 - val_precision: 0.8507 - val_recall: 0.8382\n",
      "Epoch 6/100\n",
      "34/34 - 9s - 271ms/step - accuracy: 0.8824 - f1_score: 0.8802 - loss: 0.3587 - precision: 0.9011 - recall: 0.8713 - val_accuracy: 0.8235 - val_f1_score: 0.8262 - val_loss: 0.5594 - val_precision: 0.8462 - val_recall: 0.8088\n",
      "Epoch 7/100\n",
      "34/34 - 8s - 248ms/step - accuracy: 0.9044 - f1_score: 0.9032 - loss: 0.3070 - precision: 0.9139 - recall: 0.8971 - val_accuracy: 0.8235 - val_f1_score: 0.8268 - val_loss: 0.5803 - val_precision: 0.8235 - val_recall: 0.8235\n",
      "Epoch 8/100\n",
      "34/34 - 9s - 256ms/step - accuracy: 0.8750 - f1_score: 0.8750 - loss: 0.3063 - precision: 0.8906 - recall: 0.8676 - val_accuracy: 0.8382 - val_f1_score: 0.8399 - val_loss: 0.4888 - val_precision: 0.8507 - val_recall: 0.8382\n",
      "Epoch 9/100\n",
      "34/34 - 9s - 256ms/step - accuracy: 0.9154 - f1_score: 0.9144 - loss: 0.2689 - precision: 0.9216 - recall: 0.9081 - val_accuracy: 0.8235 - val_f1_score: 0.8279 - val_loss: 0.7552 - val_precision: 0.8462 - val_recall: 0.8088\n",
      "Epoch 10/100\n",
      "34/34 - 9s - 258ms/step - accuracy: 0.9301 - f1_score: 0.9305 - loss: 0.1954 - precision: 0.9336 - recall: 0.9301 - val_accuracy: 0.7794 - val_f1_score: 0.7887 - val_loss: 1.0577 - val_precision: 0.7879 - val_recall: 0.7647\n",
      "Epoch 11/100\n",
      "34/34 - 9s - 256ms/step - accuracy: 0.8860 - f1_score: 0.8856 - loss: 0.3373 - precision: 0.8959 - recall: 0.8860 - val_accuracy: 0.8529 - val_f1_score: 0.8560 - val_loss: 0.8710 - val_precision: 0.8507 - val_recall: 0.8382\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    }
   ],
   "execution_count": 324
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:34:29.463194800Z",
     "start_time": "2025-12-14T22:39:14.415880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#transfer learning. Feature extraction\n",
    "\n",
    "\n",
    "history_feat_extract = inception_feature_extraction_model.fit(\n",
    "    transfer_model_gen,\n",
    "    validation_data = transfer_model_val_gen,\n",
    "    epochs=max_epochs,\n",
    "    callbacks=[val_loss_stop],\n",
    "    verbose = 2 #2 is one line per epoch -\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 - 9s - 275ms/step - accuracy: 0.7500 - f1_score: 0.7446 - loss: 0.7192 - precision: 0.8593 - recall: 0.6287 - val_accuracy: 0.6324 - val_f1_score: 0.5529 - val_loss: 0.9033 - val_precision: 0.7708 - val_recall: 0.5441\n",
      "Epoch 2/100\n",
      "34/34 - 9s - 268ms/step - accuracy: 0.7831 - f1_score: 0.7821 - loss: 0.6494 - precision: 0.8539 - recall: 0.6875 - val_accuracy: 0.7500 - val_f1_score: 0.7526 - val_loss: 0.6673 - val_precision: 0.8824 - val_recall: 0.6618\n",
      "Epoch 3/100\n",
      "34/34 - 9s - 271ms/step - accuracy: 0.8419 - f1_score: 0.8400 - loss: 0.5473 - precision: 0.8846 - recall: 0.7610 - val_accuracy: 0.7647 - val_f1_score: 0.7648 - val_loss: 0.7095 - val_precision: 0.8333 - val_recall: 0.6618\n",
      "Epoch 4/100\n",
      "34/34 - 9s - 273ms/step - accuracy: 0.7978 - f1_score: 0.7979 - loss: 0.5776 - precision: 0.8627 - recall: 0.7390 - val_accuracy: 0.7500 - val_f1_score: 0.7511 - val_loss: 0.6310 - val_precision: 0.8214 - val_recall: 0.6765\n",
      "Epoch 5/100\n",
      "34/34 - 9s - 270ms/step - accuracy: 0.8603 - f1_score: 0.8593 - loss: 0.4794 - precision: 0.8970 - recall: 0.7684 - val_accuracy: 0.7500 - val_f1_score: 0.7463 - val_loss: 0.6463 - val_precision: 0.8197 - val_recall: 0.7353\n",
      "Epoch 6/100\n",
      "34/34 - 10s - 287ms/step - accuracy: 0.8787 - f1_score: 0.8754 - loss: 0.4340 - precision: 0.9028 - recall: 0.8199 - val_accuracy: 0.7941 - val_f1_score: 0.7846 - val_loss: 0.5639 - val_precision: 0.8475 - val_recall: 0.7353\n",
      "Epoch 7/100\n",
      "34/34 - 10s - 292ms/step - accuracy: 0.8272 - f1_score: 0.8255 - loss: 0.4549 - precision: 0.8675 - recall: 0.7941 - val_accuracy: 0.7794 - val_f1_score: 0.7624 - val_loss: 0.6499 - val_precision: 0.8065 - val_recall: 0.7353\n",
      "Epoch 8/100\n",
      "34/34 - 10s - 287ms/step - accuracy: 0.8235 - f1_score: 0.8218 - loss: 0.4785 - precision: 0.8583 - recall: 0.7794 - val_accuracy: 0.8382 - val_f1_score: 0.8279 - val_loss: 0.5301 - val_precision: 0.8594 - val_recall: 0.8088\n",
      "Epoch 9/100\n",
      "34/34 - 10s - 289ms/step - accuracy: 0.8713 - f1_score: 0.8709 - loss: 0.4139 - precision: 0.8964 - recall: 0.8272 - val_accuracy: 0.8971 - val_f1_score: 0.9008 - val_loss: 0.4895 - val_precision: 0.8852 - val_recall: 0.7941\n",
      "Epoch 10/100\n",
      "34/34 - 10s - 302ms/step - accuracy: 0.8603 - f1_score: 0.8594 - loss: 0.3547 - precision: 0.8898 - recall: 0.8309 - val_accuracy: 0.8676 - val_f1_score: 0.8702 - val_loss: 0.5153 - val_precision: 0.8710 - val_recall: 0.7941\n",
      "Epoch 11/100\n",
      "34/34 - 11s - 330ms/step - accuracy: 0.8566 - f1_score: 0.8571 - loss: 0.4124 - precision: 0.8924 - recall: 0.8235 - val_accuracy: 0.8382 - val_f1_score: 0.8392 - val_loss: 0.4819 - val_precision: 0.8852 - val_recall: 0.7941\n",
      "Epoch 12/100\n",
      "34/34 - 10s - 297ms/step - accuracy: 0.8787 - f1_score: 0.8777 - loss: 0.3360 - precision: 0.9365 - recall: 0.8676 - val_accuracy: 0.7941 - val_f1_score: 0.7934 - val_loss: 0.5836 - val_precision: 0.8525 - val_recall: 0.7647\n",
      "Epoch 13/100\n",
      "34/34 - 10s - 295ms/step - accuracy: 0.9081 - f1_score: 0.9073 - loss: 0.3337 - precision: 0.9266 - recall: 0.8824 - val_accuracy: 0.8382 - val_f1_score: 0.8329 - val_loss: 0.5366 - val_precision: 0.8615 - val_recall: 0.8235\n",
      "Epoch 14/100\n",
      "34/34 - 10s - 284ms/step - accuracy: 0.8603 - f1_score: 0.8592 - loss: 0.3669 - precision: 0.8919 - recall: 0.8493 - val_accuracy: 0.8529 - val_f1_score: 0.8556 - val_loss: 0.4745 - val_precision: 0.8889 - val_recall: 0.8235\n",
      "Epoch 15/100\n",
      "34/34 - 10s - 291ms/step - accuracy: 0.8787 - f1_score: 0.8792 - loss: 0.3273 - precision: 0.8919 - recall: 0.8493 - val_accuracy: 0.8235 - val_f1_score: 0.8212 - val_loss: 0.5247 - val_precision: 0.8462 - val_recall: 0.8088\n",
      "Epoch 16/100\n",
      "34/34 - 10s - 296ms/step - accuracy: 0.8824 - f1_score: 0.8823 - loss: 0.3538 - precision: 0.9216 - recall: 0.8640 - val_accuracy: 0.8088 - val_f1_score: 0.8080 - val_loss: 0.5363 - val_precision: 0.8594 - val_recall: 0.8088\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    }
   ],
   "execution_count": 328
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "[Clearly specify which metrics you'll use to evaluate the model performance, and why you've chosen these metrics.]\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:34:29.463194800Z",
     "start_time": "2025-12-14T22:42:13.707162Z"
    }
   },
   "source": [
    "# Model 1\n",
    "\n",
    "#make one folder for each model to save metrics\n",
    "model_1_dir = os.path.join(hyperparam_dir,'model_1')\n",
    "if not os.path.isdir(model_1_dir):\n",
    "    os.makedirs(model_1_dir)\n",
    "\n",
    "# test accuracy on test data\n",
    "if balanced_flag:\n",
    "    test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_1_CNN.evaluate(test_generator)\n",
    "\n",
    "    #for classification report\n",
    "    true_labels = test_generator_metrics.classes\n",
    "    # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "    predicted_labels = model_1_CNN.predict(test_generator_metrics)\n",
    "\n",
    "else:\n",
    "    test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_1_CNN.evaluate(test_generator_unbalanced)\n",
    "\n",
    "    true_labels = test_generator_unbalanced_metrics.classes\n",
    "    # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "    predicted_labels = model_1_CNN.predict(test_generator_unbalanced_metrics)\n",
    "\n",
    "#convert to numerical - np.argmax directly does the job\n",
    "predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "print(f\"Model 1. Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "#save as dict for future use as well\n",
    "report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "#convert to dataframe for easy use and saving to csv\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "#save to file\n",
    "metrics_baseline_savename = os.path.join(model_1_dir,'classification_report.csv')\n",
    "\n",
    "report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "#save model as well for future use\n",
    "#save the model:\n",
    "model_1_CNN.save(os.path.join(model_1_dir,'model.keras'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 65ms/step - accuracy: 0.8605 - f1_score: 0.8535 - loss: 0.4029 - precision: 0.8706 - recall: 0.8605\n",
      "\u001B[1m86/86\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 17ms/step\n",
      "Model 1. Test Accuracy: 0.86 | Test Loss: 0.403 | Test Precision: 0.871 | Test Recall: 0.86 | Test F1 Score: 0.854:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "             0_GOOD       0.94      1.00      0.97        17\n",
      "        1_Flat loop       0.74      0.78      0.76        18\n",
      "   2_White lift-off       0.70      0.50      0.58        14\n",
      "   3_Black lift-off       0.93      1.00      0.96        13\n",
      "          4_Missing       0.82      1.00      0.90         9\n",
      "5_Short circuit MOS       1.00      0.93      0.97        15\n",
      "\n",
      "           accuracy                           0.86        86\n",
      "          macro avg       0.85      0.87      0.86        86\n",
      "       weighted avg       0.86      0.86      0.85        86\n",
      "\n"
     ]
    }
   ],
   "execution_count": 329
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:34:29.464193600Z",
     "start_time": "2025-12-14T22:42:39.776691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model 2\n",
    "\n",
    "#make one folder for each model to save metrics\n",
    "model_2_dir = os.path.join(hyperparam_dir,'model_2')\n",
    "if not os.path.isdir(model_2_dir):\n",
    "    os.makedirs(model_2_dir)\n",
    "\n",
    "# test accuracy on test data\n",
    "if balanced_flag:\n",
    "    test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_2_CNN.evaluate(test_generator)\n",
    "\n",
    "    #for classification report\n",
    "    true_labels = test_generator_metrics.classes\n",
    "    # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "    predicted_labels = model_2_CNN.predict(test_generator_metrics)\n",
    "\n",
    "else:\n",
    "    test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_2_CNN.evaluate(test_generator_unbalanced)\n",
    "\n",
    "    true_labels = test_generator_unbalanced_metrics.classes\n",
    "    # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "    predicted_labels = model_2_CNN.predict(test_generator_unbalanced_metrics)\n",
    "\n",
    "#convert to numerical - np.argmax directly does the job\n",
    "predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "print(f\"Model 2. Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "#save as dict for future use as well\n",
    "report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "#convert to dataframe for easy use and saving to csv\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "#save to file\n",
    "metrics_baseline_savename = os.path.join(model_2_dir,'classification_report.csv')\n",
    "\n",
    "report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "#save model as well for future use\n",
    "#save the model:\n",
    "model_2_CNN.save(os.path.join(model_2_dir,'model.keras'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - accuracy: 0.8140 - f1_score: 0.8180 - loss: 0.6139 - precision: 0.8214 - recall: 0.8023\n",
      "\u001B[1m86/86\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 12ms/step\n",
      "Model 2. Test Accuracy: 0.814 | Test Loss: 0.614 | Test Precision: 0.821 | Test Recall: 0.802 | Test F1 Score: 0.818:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "             0_GOOD       0.94      1.00      0.97        17\n",
      "        1_Flat loop       0.92      0.61      0.73        18\n",
      "   2_White lift-off       0.69      0.79      0.73        14\n",
      "   3_Black lift-off       0.92      0.85      0.88        13\n",
      "          4_Missing       0.53      1.00      0.69         9\n",
      "5_Short circuit MOS       1.00      0.73      0.85        15\n",
      "\n",
      "           accuracy                           0.81        86\n",
      "          macro avg       0.83      0.83      0.81        86\n",
      "       weighted avg       0.86      0.81      0.82        86\n",
      "\n"
     ]
    }
   ],
   "execution_count": 330
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:34:29.464193600Z",
     "start_time": "2025-12-14T22:43:03.543286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model 3\n",
    "\n",
    "#make one folder for each model to save metrics\n",
    "model_3_dir = os.path.join(hyperparam_dir,'model_3')\n",
    "if not os.path.isdir(model_3_dir):\n",
    "    os.makedirs(model_3_dir)\n",
    "\n",
    "# test accuracy on test data\n",
    "if balanced_flag:\n",
    "    test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_3_CNN.evaluate(test_generator)\n",
    "\n",
    "    #for classification report\n",
    "    true_labels = test_generator_metrics.classes\n",
    "    # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "    predicted_labels = model_3_CNN.predict(test_generator_metrics)\n",
    "\n",
    "else:\n",
    "    test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_3_CNN.evaluate(test_generator_unbalanced)\n",
    "\n",
    "    true_labels = test_generator_unbalanced_metrics.classes\n",
    "    # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "    predicted_labels = model_3_CNN.predict(test_generator_unbalanced_metrics)\n",
    "\n",
    "#convert to numerical - np.argmax directly does the job\n",
    "predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "print(f\"Model 3. Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "#save as dict for future use as well\n",
    "report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "#convert to dataframe for easy use and saving to csv\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "#save to file\n",
    "metrics_baseline_savename = os.path.join(model_3_dir,'classification_report.csv')\n",
    "\n",
    "report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "#save model as well for future use\n",
    "#save the model:\n",
    "model_3_CNN.save(os.path.join(model_3_dir,'model.keras'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 46ms/step - accuracy: 0.8721 - f1_score: 0.8661 - loss: 0.4320 - precision: 0.8795 - recall: 0.8488\n",
      "\u001B[1m86/86\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 12ms/step\n",
      "Model 3. Test Accuracy: 0.872 | Test Loss: 0.432 | Test Precision: 0.88 | Test Recall: 0.849 | Test F1 Score: 0.866:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "             0_GOOD       0.85      1.00      0.92        17\n",
      "        1_Flat loop       1.00      0.61      0.76        18\n",
      "   2_White lift-off       0.73      0.79      0.76        14\n",
      "   3_Black lift-off       0.92      0.92      0.92        13\n",
      "          4_Missing       0.90      1.00      0.95         9\n",
      "5_Short circuit MOS       0.88      1.00      0.94        15\n",
      "\n",
      "           accuracy                           0.87        86\n",
      "          macro avg       0.88      0.89      0.87        86\n",
      "       weighted avg       0.88      0.87      0.87        86\n",
      "\n"
     ]
    }
   ],
   "execution_count": 331
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:34:29.465192Z",
     "start_time": "2025-12-14T22:43:30.585721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model transfer feature extraction\n",
    "\n",
    "#make one folder for each model to save metrics\n",
    "model_feat_extract_dir = os.path.join(hyperparam_dir,'InceptionV3_feat_extract')\n",
    "if not os.path.isdir(model_feat_extract_dir):\n",
    "    os.makedirs(model_feat_extract_dir)\n",
    "\n",
    "# test accuracy on test data\n",
    "if balanced_flag:\n",
    "    test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_feature_extraction_model.evaluate(test_generator_color)\n",
    "\n",
    "    #for classification report\n",
    "    true_labels = test_generator_metrics.classes\n",
    "    # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "    predicted_labels = inception_feature_extraction_model.predict(test_generator_metrics_color)\n",
    "\n",
    "else:\n",
    "    test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_feature_extraction_model.evaluate(test_generator_unbalanced_color)\n",
    "\n",
    "    true_labels = test_generator_unbalanced_metrics.classes\n",
    "    # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "    predicted_labels = inception_feature_extraction_model.predict(test_generator_unbalanced_metrics_color)\n",
    "\n",
    "#convert to numerical - np.argmax directly does the job\n",
    "predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "print(f\"Feat. Extract. Model:  Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "#save as dict for future use as well\n",
    "report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "#convert to dataframe for easy use and saving to csv\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "#save to file\n",
    "metrics_baseline_savename = os.path.join(model_feat_extract_dir,'classification_report.csv')\n",
    "\n",
    "report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "#save model as well for future use\n",
    "#save the model:\n",
    "inception_feature_extraction_model.save(os.path.join(model_feat_extract_dir,'model.keras'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m11/11\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 241ms/step - accuracy: 0.7791 - f1_score: 0.7827 - loss: 0.5753 - precision: 0.8101 - recall: 0.7442\n",
      "\u001B[1m86/86\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 44ms/step\n",
      "Feat. Extract. Model:  Test Accuracy: 0.779 | Test Loss: 0.575 | Test Precision: 0.81 | Test Recall: 0.744 | Test F1 Score: 0.783:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "             0_GOOD       0.64      0.94      0.76        17\n",
      "        1_Flat loop       1.00      0.67      0.80        18\n",
      "   2_White lift-off       0.59      0.71      0.65        14\n",
      "   3_Black lift-off       0.83      0.77      0.80        13\n",
      "          4_Missing       0.90      1.00      0.95         9\n",
      "5_Short circuit MOS       1.00      0.67      0.80        15\n",
      "\n",
      "           accuracy                           0.78        86\n",
      "          macro avg       0.83      0.79      0.79        86\n",
      "       weighted avg       0.83      0.78      0.78        86\n",
      "\n"
     ]
    }
   ],
   "execution_count": 332
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "[Compare the performance of your model(s) against the baseline model. Discuss any improvements or setbacks and the reasons behind them.]\n",
    "\n",
    "A table comparing the performances of different models and hyperparameter settings can be found in the github (Model_Performance_overview.xls or Model_Performance_overview.csv).\n",
    "\n",
    "Some results stand out:\n",
    "\n",
    "* data augmentation seems to lower model performance across the board even when we see overfitting in training. The likely reason is that the data itself is very regular without a lot of orientation of the features in the images. Therefore, we well adjust data augmentation in future to exlude image flipping etc.\n",
    "* The transfer learning model performs worse than the 3 relatively simple models. Especially for low image resolutions. The most likely reason is that, as of now we only use feature extraction. For any image size that the model was not originally trained on this will very likely mean a bad performance. For higher resolutions the transfer learning model performs better in comparison\n",
    "* Higher image resolution does not really improve model performance.\n",
    "\n",
    "Some things are still missing in the analysis / evaluation and will be added in the near future:\n",
    "\n",
    "* Transfer learning models with fine tuning\n",
    "* Different transfer learning base architectures\n",
    "* When a best model is found we will tackle the task of identifying the drift label class\n",
    "* More finetuning of hyperparameters for few selected models\n",
    "* class weighting instead of balanced dataset (balanced dataset is very small)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
