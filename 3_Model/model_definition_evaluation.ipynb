{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:07.566346Z",
     "start_time": "2026-01-21T18:12:07.431769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#reset all variables when running again to avoid any mistakes\n",
    "\n",
    "%reset -f"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:10.688630Z",
     "start_time": "2026-01-21T18:12:07.576350Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import gc\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.src.legacy.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from matplotlib.ticker import StrMethodFormatter\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Selection\n",
    "\n",
    "We are considering and evaluating self-defined CNN and pretrained CNNs. Convolutional Neural Networks are proven to work well for the task of image classification. Most likely transformer models could potentially have an even better performance but it is highly likely that the added complexity will not justify the potential increase in performance.\n",
    "\n",
    "Transfer learning based on different model architectures is also performed. Different training methods are utilized and comppared\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameters\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# choose main hyperparameters here\n",
    "\n",
    "#data / feature selections\n",
    "balanced_flag = False\n",
    "\n",
    "#Traing data splits :\n",
    "test_split = 0.2\n",
    "val_split = 0.20 # remember - this is fractional  after the test data has been split from the initial balanced sub dataset\n",
    "\n",
    "# image parameters\n",
    "target_size = (299,299) #pixel size to load img\n",
    "#efficientnet V2S recommended img size 384 , https://www.kaggle.com/models/google/efficientnet-v2\n",
    "#Inception V3 recommended image size 299x299\n",
    "\n",
    "#select data augmentation\n",
    "aug_flag = True\n",
    "#augmentation params\n",
    "horizontal_flip=False\n",
    "vertical_flip=False\n",
    "rotation_range=15\n",
    "shear_range= 1\n",
    "zoom_range = 0.07\n",
    "\n",
    "#training\n",
    "max_epochs = 100\n",
    "loss_stop_patience = 7\n",
    "learningRate = 0.001\n",
    "\n",
    "loss_stop_patience_multi = [5,6,8] #patience in each step\n",
    "\n",
    "#class weights\n",
    "Use_class_weights = True\n",
    "\n",
    "#early stopping\n",
    "StoppingSelector = 'val_loss' #valid values: 'val_loss','val_f1'\n",
    "\n",
    "#flags for models to include\n",
    "model1_flag = False\n",
    "model2_flag = False\n",
    "model3_flag = False\n",
    "feature_extract_flag = False\n",
    "fine_tune_flag = False\n",
    "full_fine_tune_flag = False\n",
    "multi_phase_fine_tune_flag = False\n",
    "reducedClassNumber_flag = False\n",
    "EfficientNet_Flag = False\n",
    "\n",
    "#set batch size according to balanc selection\n",
    "if balanced_flag:\n",
    "    batch_size = 8 #later player around with batch size to see how it affects performance\n",
    "else:\n",
    "    batch_size = 32 #hopefully speeds up training"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# specify the model loss function\n",
    "\n",
    "#options: 'normal' :   tf.keras.losses.CategoricalCrossentropy()    , 'focal' : tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "lossSelect = 'normal'\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:10.743921Z",
     "start_time": "2026-01-21T18:12:10.713345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#select optimizer\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:10.752779Z",
     "start_time": "2026-01-21T18:12:10.748924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#make customizable string to add to dir for testing\n",
    "if lossSelect == 'focal':\n",
    "    custom_save_str = '_focalLoss'\n",
    "else:\n",
    "    custom_save_str = ''"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "[Describe any additional feature engineering you've performed beyond what was done for the baseline model.]\n",
    "\n",
    "We test data augmentation (to varying degrees), dataset balancing and class weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:10.773672Z",
     "start_time": "2026-01-21T18:12:10.758783Z"
    }
   },
   "source": [
    "# Path Definitions to relevant data + data loading\n",
    "\n",
    "base_file_path = 'C:/Users/nikoLocal/Documents/Opencampus/Machine_Vision_challenge_data/'\n",
    "image_path = base_file_path + '/input_train/input_train'\n",
    "\n",
    "label_csv_name = 'Y_train_eVW9jym.csv'\n",
    "\n",
    "#Loading .csv data to dataframes\n",
    "train_df = pd.read_csv(os.path.join(base_file_path, label_csv_name))\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:10.799587Z",
     "start_time": "2026-01-21T18:12:10.779663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#DataFrame Preprocessing\n",
    "\n",
    "#add another column to the dataframe according to dictionaries to map Labels correctly to numbers\n",
    "dict_numbers = {'GOOD': 0,'Boucle plate':1,'Lift-off blanc':2,'Lift-off noir':3,'Missing':4,'Short circuit MOS':5}\n",
    "dict_strings = {'GOOD': '0_GOOD','Boucle plate':'1_Flat loop','Lift-off blanc':'2_White lift-off','Lift-off noir':'3_Black lift-off','Missing':'4_Missing','Short circuit MOS':'5_Short circuit MOS'}\n",
    "# for Test Data (\"random submission\" dataframe)\n",
    "dict_strings_sub = {0: '0_GOOD',1:'1_Flat loop',2:'2_White lift-off',3:'3_Black lift-off',4:'4_Missing',5:'5_Short circuit MOS',6:'6_Drift'}\n",
    "\n",
    "#list of all labels in the data\n",
    "label_list = ['0_GOOD','1_Flat loop','2_White lift-off','3_Black lift-off','4_Missing','5_Short circuit MOS']\n",
    "\n",
    "#create new columns in DFs via .map() method\n",
    "train_df['LabelNum'] = train_df['Label'].map(dict_numbers)\n",
    "train_df['LabelStr'] = train_df['Label'].map(dict_strings)\n",
    "\n",
    "#number of classes\n",
    "num_classes = len(label_list)\n",
    "\n",
    "# get counts of label with the least entries\n",
    "countList = train_df['LabelStr'].value_counts()\n",
    "minCounts = countList.min()\n",
    "\n",
    "BalancedDF = pd.DataFrame()\n",
    "#concat sampled dataframes for each included label\n",
    "for i in range(num_classes):\n",
    "    BalancedDF = pd.concat([BalancedDF,train_df[train_df['LabelStr'] == label_list[i]].sample(n=minCounts)],axis=0)\n",
    "\n",
    "#split dataframe according to fractional test size\n",
    "train_df_balanced, test_df_balanced = train_test_split(BalancedDF, test_size=test_split, random_state=42) #keep random state constant to ensure\n",
    "\n",
    "train_df_train, train_df_test = train_test_split(train_df, test_size=test_split, random_state=42) #keep random state constant to ensure"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:10.815287Z",
     "start_time": "2026-01-21T18:12:10.810591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#test if worked as intended\n",
    "print('Balanced DF Label Counts:')\n",
    "print(BalancedDF['LabelStr'].value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced DF Label Counts:\n",
      "LabelStr\n",
      "0_GOOD                 71\n",
      "1_Flat loop            71\n",
      "2_White lift-off       71\n",
      "3_Black lift-off       71\n",
      "4_Missing              71\n",
      "5_Short circuit MOS    71\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:10.847363Z",
     "start_time": "2026-01-21T18:12:10.843296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#test if worked as intended\n",
    "print('DF Label Counts:')\n",
    "print(train_df['LabelStr'].value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF Label Counts:\n",
      "LabelStr\n",
      "4_Missing              6472\n",
      "0_GOOD                 1235\n",
      "2_White lift-off        270\n",
      "5_Short circuit MOS     126\n",
      "3_Black lift-off        104\n",
      "1_Flat loop              71\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:10.874824Z",
     "start_time": "2026-01-21T18:12:10.866370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#compute class weights for use of unbalanced datasets\n",
    "\n",
    "class_numbers = np.unique(train_df_train['LabelNum'])\n",
    "\n",
    "class_weights_unb = compute_class_weight(class_weight='balanced' ,classes = class_numbers,y=train_df_train['LabelNum'])\n",
    "class_weights_b = compute_class_weight(class_weight='balanced' ,classes = class_numbers,y=train_df_balanced['LabelNum'])\n",
    "equal_weights = np.ones(num_classes)\n",
    "\n",
    "#make dicts that can be used by keras\n",
    "class_w_unb_dict = dict(zip(class_numbers, class_weights_unb))\n",
    "class_w_b_dict = dict(zip(class_numbers, class_weights_b))\n",
    "class_w_equal_dict = dict(zip(class_numbers, equal_weights))"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:10.899986Z",
     "start_time": "2026-01-21T18:12:10.884822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# make a dataset with only labels: 0_Good, 1_Flat loop and 2_Defective\n",
    "# in order to test whether flat loop can be recognized with high precision by a NN\n",
    "\n",
    "#make a deepcopy of existing df\n",
    "df_reduced_labels = train_df.copy(deep = True)\n",
    "\n",
    "#dict_strings = {'GOOD': '0_GOOD','Boucle plate':'1_Flat loop','Lift-off blanc':'2_White lift-off','Lift-off noir':'3_Black lift-off','Missing':'4_Missing','Short circuit MOS':'5_Short circuit MOS'}\n",
    "\n",
    "df_reduced_labels.loc[df_reduced_labels['LabelStr'] == '2_White lift-off', 'LabelStr'] = '2_Defective'\n",
    "df_reduced_labels.loc[df_reduced_labels['LabelStr'] == '3_Black lift-off', 'LabelStr'] = '2_Defective'\n",
    "df_reduced_labels.loc[df_reduced_labels['LabelStr'] == '4_Missing', 'LabelStr'] = '2_Defective'\n",
    "df_reduced_labels.loc[df_reduced_labels['LabelStr'] == '5_Short circuit MOS', 'LabelStr'] = '2_Defective'\n",
    "\n",
    "df_reduced_labels.loc[df_reduced_labels['LabelNum'] == 2, 'LabelNum'] = 2\n",
    "df_reduced_labels.loc[df_reduced_labels['LabelNum'] == 3, 'LabelNum'] = 2\n",
    "df_reduced_labels.loc[df_reduced_labels['LabelNum'] == 4, 'LabelNum'] = 2\n",
    "df_reduced_labels.loc[df_reduced_labels['LabelNum'] == 5, 'LabelNum'] = 2\n",
    "\n",
    "# split into training and test data\n",
    "df_reduced_labels_train, df_reduced_labels_test = train_test_split(df_reduced_labels, test_size=test_split, random_state=42) #keep random state constant to ensure\n",
    "\n",
    "#class weights for this dataset\n",
    "class_numbers_redLabel = np.unique(df_reduced_labels_train['LabelNum'])\n",
    "\n",
    "class_weights_redLabel = compute_class_weight(class_weight='balanced' ,classes = class_numbers_redLabel,y=df_reduced_labels_train['LabelNum'])\n",
    "\n",
    "#make dicts that can be used by keras\n",
    "class_weights_redLabel_dict = dict(zip(class_numbers_redLabel, class_weights_redLabel))\n",
    "\n",
    "num_classes_redLabel = 3\n",
    "\n",
    "label_list_reduced = ['0_Good','1_Flat Loop','2_Defective']"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:10.916521Z",
     "start_time": "2026-01-21T18:12:10.907990Z"
    }
   },
   "cell_type": "code",
   "source": "df_reduced_labels['LabelStr'].value_counts()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelStr\n",
       "2_Defective    6972\n",
       "0_GOOD         1235\n",
       "1_Flat loop      71\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "[Discuss any hyperparameter tuning methods you've applied, such as Grid Search or Random Search, and the rationale behind them.]\n",
    "So far we have done hyperparameters variation \"by hand\" only. Some parameters, such as the image size and data augmentation have been systematically varied and the effects on model performance noted.\n",
    "\n",
    "Automated hyperparameter variation has not been utilized in the scope of this project. Although we are sure that there is enormous potential for improvement there."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# make a unique string (name) to save model and evaluation to file\n",
    "# incorporate most important hyperparameters\n",
    "# make a subfolder for one set of hyperparameters for more tidy folder and file structure\n",
    "\n",
    "if aug_flag:\n",
    "    augmentation_str = 'Aug'\n",
    "else:\n",
    "    augmentation_str = 'NoAug'\n",
    "\n",
    "if balanced_flag:\n",
    "    balance_str = 'balanced'\n",
    "else:\n",
    "    balance_str = 'unbalanced'\n",
    "\n",
    "#class weights\n",
    "if Use_class_weights:\n",
    "    Cweights_str = '_Cweights'\n",
    "else:\n",
    "    Cweights_str = ''\n",
    "\n",
    "hyperparam_name = 'ImgSz_{}_{}_{}{}{}'.format(target_size[0],augmentation_str,balance_str,Cweights_str,custom_save_str)\n",
    "hyperparam_dir = os.path.join(base_file_path,'model_evaluation')\n",
    "hyperparam_dir = os.path.join(hyperparam_dir,hyperparam_name)\n",
    "#check if folder exists - if not create it\n",
    "if not os.path.isdir(hyperparam_dir):\n",
    "    os.makedirs(hyperparam_dir)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# initialize ImageDataGenerators\n",
    "# use ImageDataGen because it has method flow_from_dataframe() that works really well together with pandas dataframes\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# although deprecated the functionality can be used as discussed in feedback session\n",
    "\n",
    "# HYPERPARAMTERS ########\n",
    "class_mode = 'categorical' # how to store labels - either categorical (one-hot encoding) or as numbers\n",
    "#class_mode = 'input'\n",
    "labelCol = 'LabelStr'\n",
    "#########################\n",
    "\n",
    "#normalize pixel intensities\n",
    "rescale = 1.0/255.0\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=0.0,\n",
    "    shear_range=0.0,\n",
    "    rescale=rescale,\n",
    "    validation_split=val_split)\n",
    "\n",
    "datagen_augmentation = ImageDataGenerator(\n",
    "    horizontal_flip=horizontal_flip,\n",
    "    vertical_flip=vertical_flip,\n",
    "    rotation_range=rotation_range,\n",
    "    shear_range= shear_range,\n",
    "    zoom_range = zoom_range,\n",
    "    rescale=rescale,\n",
    "    validation_split=val_split)\n",
    "\n",
    "datagen_test = ImageDataGenerator(\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=0.0,\n",
    "    shear_range=0.0,\n",
    "    rescale=rescale,\n",
    "    validation_split=0.0)\n",
    "\n",
    "\n",
    "##########################################################\n",
    "\n",
    "#unbalanced datasets\n",
    "\n",
    "train_generator_unbalanced = datagen.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_unbalanced_val = datagen.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "train_generator_unbalanced_aug = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_unbalanced_val_aug = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "test_generator_unbalanced = datagen_test.flow_from_dataframe(\n",
    "    train_df_test,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "test_generator_unbalanced_metrics = datagen_test.flow_from_dataframe(\n",
    "    train_df_test,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_val = datagen.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "train_generator_aug = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_aug_val = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "test_generator = datagen_test.flow_from_dataframe(\n",
    "    test_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "test_generator_metrics = datagen_test.flow_from_dataframe(\n",
    "    test_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1,\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "# generators for transfer learning - color mode is color here. Pretrained models expect color input\n",
    "\n",
    "train_generator_unbalanced_color = datagen.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_unbalanced_val_color = datagen.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "test_generator_unbalanced_color = datagen_test.flow_from_dataframe(\n",
    "    train_df_test,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_unbalanced_aug_color = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "test_generator_unbalanced_metrics_color = datagen_test.flow_from_dataframe(\n",
    "    train_df_test,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_color = datagen.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_val_color = datagen.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "train_generator_aug_color = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_aug_val_color = datagen_augmentation.flow_from_dataframe(\n",
    "    train_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "test_generator_color = datagen_test.flow_from_dataframe(\n",
    "    test_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "test_generator_metrics_color = datagen_test.flow_from_dataframe(\n",
    "    test_df_balanced,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#dataset for reduced class number\n",
    "\n",
    "train_generator_redLabel = datagen.flow_from_dataframe(\n",
    "    df_reduced_labels_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_redLabel_val = datagen.flow_from_dataframe(\n",
    "    df_reduced_labels_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation')\n",
    "\n",
    "test_generator_redLabel = datagen_test.flow_from_dataframe(\n",
    "    df_reduced_labels_test,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "train_generator_redLabel_aug = datagen_augmentation.flow_from_dataframe(\n",
    "    df_reduced_labels_train,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training')\n",
    "\n",
    "test_generator_redLabel_metrics = datagen_test.flow_from_dataframe(\n",
    "    df_reduced_labels_test,\n",
    "    image_path,\n",
    "    x_col='filename',\n",
    "    y_col=labelCol,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1,\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement the final model(s) you've selected based on the above steps.]\n",
    "\n",
    "Here we implement various self-defined and preexisting models to compare their performance. All models are always initialized but not always trained to save on execution time."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# build a model to be used as baseline model\n",
    "# use \"simplest\" CNN as baseline\n",
    "\n",
    "model_1_CNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input((target_size[0], target_size[1], 1)),  #image are greyscale - so in total dim (width,height,1)\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model_2_CNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input((target_size[0], target_size[1], 1)),  #image are greyscale - so in total dim (width,height,1)\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model_3_CNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input((target_size[0], target_size[1], 1)),  #image are greyscale - so in total dim (width,height,1)\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model_1_CNN.summary()\n",
    "model_2_CNN.summary()\n",
    "model_3_CNN.summary()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#loss\n",
    "\n",
    "if balanced_flag:\n",
    "    focal_alpha = class_weights_b\n",
    "else:\n",
    "    focal_alpha = class_weights_unb\n",
    "\n",
    "# callback that monitors validation accuracy / loss\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "\n",
    "\n",
    "match StoppingSelector:\n",
    "    case 'val_loss':\n",
    "        StopCallback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.01,\n",
    "            patience= loss_stop_patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose = 2,\n",
    "            start_from_epoch = 1\n",
    "        )\n",
    "    case 'val_f1':\n",
    "        StopCallback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_f1_score',\n",
    "            min_delta=0.005,\n",
    "            patience= loss_stop_patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose = 2,\n",
    "            mode='max'\n",
    "        )\n",
    "\n",
    "class ResetValLossOnTrainBegin(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        #set val loss to a high value in case there is a history left\n",
    "        logs[\"val_loss\"] =  1e3\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load weights gained by training from medical MRI data into inception V3\n",
    "\n",
    "# Load pre-trained InceptionV3 with correct input size\n",
    "\n",
    "#path to weights\n",
    "weights_subpath = 'pre_trained_models/radiology_net/InceptionV3.pth'\n",
    "medical_weights_path = os.path.join(base_file_path,weights_subpath)\n",
    "\n",
    "#convert weights to keras format\n",
    "\n",
    "#base_transfer_model_medical = tf.keras.applications.InceptionV3(\n",
    "#    weights=medical_weights_path,\n",
    "#    include_top=False,\n",
    "#    input_shape=(target_size[0], target_size[0], 3)\n",
    "#)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Transfer learning model - feature extraction\n",
    "\n",
    "# Load pre-trained InceptionV3 with correct input size\n",
    "base_transfer_model = tf.keras.applications.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(target_size[0], target_size[0], 3)\n",
    ")\n",
    "\n",
    "# Freeze all layers for feature extraction\n",
    "base_transfer_model.trainable = False\n",
    "\n",
    "# Simple classification head\n",
    "# - GlobalAveragePooling2D reduces spatial dimensions\n",
    "# - Final Dense layer maps to class probabilities\n",
    "inception_feature_extraction_model = tf.keras.Sequential([\n",
    "    base_transfer_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "#select based on Str\n",
    "if lossSelect == 'normal':\n",
    "    loss_fun_feat = tf.keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    loss_fun_feat = tf.keras.losses.CategoricalFocalCrossentropy(alpha = focal_alpha,gamma = 2)\n",
    "\n",
    "inception_feature_extraction_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "    loss=loss_fun_feat,\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "inception_feature_extraction_model.summary()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# inception model with last 20 layers unfrozen\n",
    "\n",
    "# Load pre-trained InceptionV3 with correct input size\n",
    "base_transfer_model_2 = tf.keras.applications.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(target_size[0], target_size[0], 3)\n",
    ")\n",
    "\n",
    "# Freeze all layers except last few blocks\n",
    "base_transfer_model_2.trainable = True\n",
    "for layer in base_transfer_model_2.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Simple classification head\n",
    "# - GlobalAveragePooling2D reduces spatial dimensions\n",
    "# - Final Dense layer maps to class probabilities\n",
    "inception_fine_tune_model = tf.keras.Sequential([\n",
    "    base_transfer_model_2,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "#select based on Str\n",
    "if lossSelect == 'normal':\n",
    "    loss_fun_fine = tf.keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    loss_fun_fine = tf.keras.losses.CategoricalFocalCrossentropy(alpha = focal_alpha,gamma = 2)\n",
    "\n",
    "inception_fine_tune_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=loss_fun_fine,\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "inception_fine_tune_model.summary()"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:18.338623Z",
     "start_time": "2026-01-21T18:12:17.134453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# inception model with all layers unfrozen\n",
    "\n",
    "# Load pre-trained InceptionV3 with correct input size\n",
    "base_transfer_model_3 = tf.keras.applications.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(target_size[0], target_size[0], 3)\n",
    ")\n",
    "\n",
    "# Freeze all layers except last few blocks\n",
    "base_transfer_model_3.trainable = True\n",
    "#for layer in base_transfer_model_3.layers[:-20]:\n",
    "#    layer.trainable = False\n",
    "\n",
    "# Simple classification head\n",
    "# - GlobalAveragePooling2D reduces spatial dimensions\n",
    "# - Final Dense layer maps to class probabilities\n",
    "inception_full_fine_tune_model = tf.keras.Sequential([\n",
    "    base_transfer_model_3,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2), #do I need to keep this ?\n",
    "    tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "])\n",
    "\n",
    "#select based on Str\n",
    "if lossSelect == 'normal':\n",
    "    loss_fun_full_fine = tf.keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    loss_fun_full_fine = tf.keras.losses.CategoricalFocalCrossentropy(alpha = focal_alpha,gamma = 2)\n",
    "\n",
    "inception_full_fine_tune_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=loss_fun_full_fine,\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "inception_full_fine_tune_model.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_5\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inception_v3 (\u001B[38;5;33mFunctional\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m2048\u001B[0m)     │    \u001B[38;5;34m21,802,784\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │        \u001B[38;5;34m12,294\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inception_v3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,294</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m21,815,078\u001B[0m (83.22 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,815,078</span> (83.22 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m21,780,646\u001B[0m (83.09 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,780,646</span> (83.09 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m34,432\u001B[0m (134.50 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,432</span> (134.50 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:19.546Z",
     "start_time": "2026-01-21T18:12:18.365626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# inception model where layers are successively unfrozen during training\n",
    "\n",
    "# Load pre-trained InceptionV3 with correct input size\n",
    "base_transfer_model_4 = tf.keras.applications.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(target_size[0], target_size[0], 3)\n",
    ")\n",
    "\n",
    "# Freeze all layers of Inception model\n",
    "base_transfer_model_4.trainable = False\n",
    "\n",
    "# Simple classification head\n",
    "# - GlobalAveragePooling2D reduces spatial dimensions\n",
    "# - Final Dense layer maps to class probabilities\n",
    "inception_multiPhase_fine_tune_model = tf.keras.Sequential([\n",
    "    base_transfer_model_4,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "])\n",
    "\n",
    "#select based on Str\n",
    "if lossSelect == 'normal':\n",
    "    loss_fun_multi_fine = tf.keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    loss_fun_multi_fine = tf.keras.losses.CategoricalFocalCrossentropy(alpha = focal_alpha,gamma = 2)\n",
    "\n",
    "inception_multiPhase_fine_tune_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), #start with \"normal\" learning rate\n",
    "    loss=loss_fun_multi_fine,\n",
    "    weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "inception_multiPhase_fine_tune_model.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_6\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inception_v3 (\u001B[38;5;33mFunctional\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m2048\u001B[0m)     │    \u001B[38;5;34m21,802,784\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │        \u001B[38;5;34m12,294\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inception_v3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,294</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m21,815,078\u001B[0m (83.22 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,815,078</span> (83.22 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m12,294\u001B[0m (48.02 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,294</span> (48.02 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m21,802,784\u001B[0m (83.17 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> (83.17 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:21.010009Z",
     "start_time": "2026-01-21T18:12:19.585003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# inception model where layers are sucessively unfrozen during training - for reduced class Number\n",
    "\n",
    "# Load pre-trained InceptionV3 with correct input size\n",
    "base_transfer_model_5 = tf.keras.applications.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(target_size[0], target_size[0], 3)\n",
    ")\n",
    "\n",
    "# Freeze all layers except last few blocks\n",
    "base_transfer_model_5.trainable = False\n",
    "#for layer in base_transfer_model_3.layers[:-20]:\n",
    "#    layer.trainable = False\n",
    "\n",
    "# Simple classification head\n",
    "# - GlobalAveragePooling2D reduces spatial dimensions\n",
    "# - Final Dense layer maps to class probabilities\n",
    "inception_multiPhase_redLabel = tf.keras.Sequential([\n",
    "    base_transfer_model_5,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2), #do I need to keep this ?\n",
    "    tf.keras.layers.Dense(num_classes_redLabel, activation = 'softmax')\n",
    "])\n",
    "\n",
    "#select based on Str\n",
    "if lossSelect == 'normal':\n",
    "    loss_fun_multi_redLabel = tf.keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    loss_fun_multi_redLabel = tf.keras.losses.CategoricalFocalCrossentropy(alpha = focal_alpha,gamma = 2)\n",
    "\n",
    "inception_multiPhase_redLabel.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), #start with \"normal\" learning rate\n",
    "    loss=loss_fun_multi_redLabel,\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "inception_multiPhase_redLabel.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_7\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inception_v3 (\u001B[38;5;33mFunctional\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m2048\u001B[0m)     │    \u001B[38;5;34m21,802,784\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3\u001B[0m)              │         \u001B[38;5;34m6,147\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inception_v3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,147</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m21,808,931\u001B[0m (83.19 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,808,931</span> (83.19 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m6,147\u001B[0m (24.01 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,147</span> (24.01 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m21,802,784\u001B[0m (83.17 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> (83.17 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:22.953075Z",
     "start_time": "2026-01-21T18:12:21.038012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# EfficientNet model where layers are sucessively unfrozen during training\n",
    "\n",
    "# Load pre-trained InceptionV3 with correct input size\n",
    "base_model_efficientNet = tf.keras.applications.EfficientNetV2S(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    include_preprocessing=True,\n",
    "    input_shape=(target_size[0], target_size[0], 3)\n",
    ")\n",
    "\n",
    "# Freeze all layers except last few blocks\n",
    "base_model_efficientNet.trainable = False\n",
    "#for layer in base_transfer_model_3.layers[:-20]:\n",
    "#    layer.trainable = False\n",
    "\n",
    "# Simple classification head\n",
    "# - GlobalAveragePooling2D reduces spatial dimensions\n",
    "# - Final Dense layer maps to class probabilities\n",
    "EfficientNet_multiPhase_model = tf.keras.Sequential([\n",
    "    base_model_efficientNet,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2), #do I need to keep this ?\n",
    "    tf.keras.layers.Dense(num_classes, activation = 'softmax')\n",
    "])\n",
    "\n",
    "#select based on Str\n",
    "if lossSelect == 'normal':\n",
    "    loss_fun_EfficientNet = tf.keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    loss_fun_EfficientNet = tf.keras.losses.CategoricalFocalCrossentropy(alpha = focal_alpha,gamma = 2)\n",
    "\n",
    "EfficientNet_multiPhase_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), #start with \"normal\" learning rate\n",
    "    loss=loss_fun_EfficientNet,\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "EfficientNet_multiPhase_model.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_8\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetv2-s (\u001B[38;5;33mFunctional\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m, \u001B[38;5;34m10\u001B[0m, \u001B[38;5;34m1280\u001B[0m)   │    \u001B[38;5;34m20,331,360\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_5      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1280\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1280\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)              │         \u001B[38;5;34m7,686\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetv2-s (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,331,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_5      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,686</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m20,339,046\u001B[0m (77.59 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,339,046</span> (77.59 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m7,686\u001B[0m (30.02 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,686</span> (30.02 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m20,331,360\u001B[0m (77.56 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,331,360</span> (77.56 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:12:22.980651Z",
     "start_time": "2026-01-21T18:12:22.963072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compile Models\n",
    "\n",
    "#select based on Str\n",
    "if lossSelect == 'normal':\n",
    "    loss_fun_1 = tf.keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    loss_fun_1 = tf.keras.losses.CategoricalFocalCrossentropy(alpha = focal_alpha,gamma = 2)\n",
    "\n",
    "model_1_CNN.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "    loss=loss_fun_1,\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "#select based on Str\n",
    "if lossSelect == 'normal':\n",
    "    loss_fun_2 = tf.keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    loss_fun_2 = tf.keras.losses.CategoricalFocalCrossentropy(alpha = focal_alpha,gamma = 2)\n",
    "\n",
    "model_2_CNN.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "    loss=loss_fun_2,\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "#select based on Str\n",
    "if lossSelect == 'normal':\n",
    "    loss_fun_3 = tf.keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    loss_fun_3 = tf.keras.losses.CategoricalFocalCrossentropy(alpha = focal_alpha,gamma = 2)\n",
    "\n",
    "model_3_CNN.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "    loss=loss_fun_3,\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "    #metrics=[\"accuracy\",'precision',]\n",
    "    #metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    "    weighted_metrics =[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    ")\n",
    "\n",
    "#F1 average parameter needs to be anything other than None if using linewise output when fiting the model...\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#select model datasets based on flags\n",
    "\n",
    "#for model1 - 3\n",
    "if balanced_flag:\n",
    "    CNN_model_val_gen = train_generator_val\n",
    "    if aug_flag:\n",
    "        CNN_model_gen = train_generator_aug\n",
    "    else:\n",
    "        CNN_model_gen = train_generator\n",
    "    #class weights\n",
    "    class_weights_training = class_weights_b\n",
    "else:\n",
    "    CNN_model_val_gen = train_generator_unbalanced_val\n",
    "    if aug_flag:\n",
    "        CNN_model_gen = train_generator_unbalanced_aug\n",
    "    else:\n",
    "        CNN_model_gen = train_generator_unbalanced\n",
    "\n",
    "#for transfer learning\n",
    "if balanced_flag:\n",
    "    transfer_model_val_gen = train_generator_val_color\n",
    "    if aug_flag:\n",
    "        transfer_model_gen = train_generator_aug_color\n",
    "    else:\n",
    "        transfer_model_gen = train_generator_color\n",
    "else:\n",
    "    transfer_model_val_gen = train_generator_unbalanced_val_color\n",
    "    if aug_flag:\n",
    "        transfer_model_gen = train_generator_unbalanced_aug_color\n",
    "    else:\n",
    "        transfer_model_gen = train_generator_unbalanced_color\n",
    "\n",
    "\n",
    "if Use_class_weights:\n",
    "    if balanced_flag:\n",
    "        class_weights_training = class_w_b_dict\n",
    "    else:\n",
    "        class_weights_training = class_w_unb_dict\n",
    "else:\n",
    "    class_weights_training = class_w_equal_dict\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the next sections models are trained based on whether their corresponding flag was set to True for training"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#model 1\n",
    "if model1_flag:\n",
    "\n",
    "    history_1 = model_1_CNN.fit(\n",
    "    CNN_model_gen,\n",
    "    validation_data = CNN_model_val_gen,\n",
    "    epochs=max_epochs,\n",
    "    class_weight = class_weights_training,\n",
    "    callbacks=[StopCallback,ResetValLossOnTrainBegin()],\n",
    "    verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Model 2\n",
    "\n",
    "if model2_flag:\n",
    "    history_2 = model_2_CNN.fit(\n",
    "        CNN_model_gen,\n",
    "        validation_data = CNN_model_val_gen,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback,ResetValLossOnTrainBegin()],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:00:33.142975Z",
     "start_time": "2026-01-21T20:37:38.581411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Model 3\n",
    "\n",
    "if model3_flag:\n",
    "    # model_3_CNN.fit(\n",
    "    #     CNN_model_gen,\n",
    "    #     validation_data = CNN_model_val_gen,\n",
    "    #     epochs=3,\n",
    "    #     class_weight = class_weights_training,\n",
    "    #     callbacks=[StopCallback,ResetValLossOnTrainBegin()],\n",
    "    #     verbose = 2 #2 is one line per epoch -\n",
    "    # )\n",
    "\n",
    "    history_3 = model_3_CNN.fit(\n",
    "        CNN_model_gen,\n",
    "        validation_data = CNN_model_val_gen,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback,ResetValLossOnTrainBegin()],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "166/166 - 177s - 1s/step - accuracy: 0.5454 - f1_score: 0.5461 - loss: 1.2161 - precision: 0.7770 - recall: 0.4018 - val_accuracy: 0.9645 - val_f1_score: 0.9690 - val_loss: 0.4443 - val_precision: 0.9744 - val_recall: 0.8625\n",
      "Epoch 2/100\n",
      "166/166 - 174s - 1s/step - accuracy: 0.8723 - f1_score: 0.8720 - loss: 0.4077 - precision: 0.8914 - recall: 0.8427 - val_accuracy: 0.9751 - val_f1_score: 0.9767 - val_loss: 0.1192 - val_precision: 0.9758 - val_recall: 0.9743\n",
      "Epoch 3/100\n",
      "166/166 - 171s - 1s/step - accuracy: 0.8923 - f1_score: 0.8919 - loss: 0.3425 - precision: 0.9056 - recall: 0.8716 - val_accuracy: 0.9607 - val_f1_score: 0.9702 - val_loss: 0.2540 - val_precision: 0.9845 - val_recall: 0.8640\n",
      "Epoch 4/100\n",
      "166/166 - 175s - 1s/step - accuracy: 0.9041 - f1_score: 0.9035 - loss: 0.2755 - precision: 0.9100 - recall: 0.8899 - val_accuracy: 0.9622 - val_f1_score: 0.9684 - val_loss: 0.1803 - val_precision: 0.9644 - val_recall: 0.9622\n",
      "Epoch 5/100\n",
      "166/166 - 169s - 1s/step - accuracy: 0.9131 - f1_score: 0.9128 - loss: 0.2733 - precision: 0.9215 - recall: 0.9030 - val_accuracy: 0.9826 - val_f1_score: 0.9843 - val_loss: 0.1140 - val_precision: 0.9833 - val_recall: 0.9804\n",
      "Epoch 6/100\n",
      "166/166 - 171s - 1s/step - accuracy: 0.9318 - f1_score: 0.9314 - loss: 0.1789 - precision: 0.9358 - recall: 0.9311 - val_accuracy: 0.9834 - val_f1_score: 0.9846 - val_loss: 0.0740 - val_precision: 0.9849 - val_recall: 0.9834\n",
      "Epoch 7/100\n",
      "166/166 - 170s - 1s/step - accuracy: 0.9143 - f1_score: 0.9138 - loss: 0.2765 - precision: 0.9205 - recall: 0.9076 - val_accuracy: 0.9184 - val_f1_score: 0.9456 - val_loss: 0.1880 - val_precision: 0.9224 - val_recall: 0.9154\n",
      "Epoch 8/100\n",
      "166/166 - 167s - 1s/step - accuracy: 0.9411 - f1_score: 0.9411 - loss: 0.1876 - precision: 0.9461 - recall: 0.9358 - val_accuracy: 0.9789 - val_f1_score: 0.9797 - val_loss: 0.0767 - val_precision: 0.9796 - val_recall: 0.9781\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.579551Z",
     "start_time": "2026-01-21T21:00:33.171984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#transfer learning. Feature extraction\n",
    "\n",
    "if feature_extract_flag:\n",
    "    history_feat_extract = inception_feature_extraction_model.fit(\n",
    "        transfer_model_gen,\n",
    "        validation_data = transfer_model_val_gen,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback,ResetValLossOnTrainBegin()],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "166/166 - 248s - 1s/step - accuracy: 0.5702 - f1_score: 0.5664 - loss: 1.1982 - precision: 0.7259 - recall: 0.3830 - val_accuracy: 0.9199 - val_f1_score: 0.9373 - val_loss: 0.2503 - val_precision: 0.9643 - val_recall: 0.8761\n",
      "Epoch 2/100\n",
      "166/166 - 241s - 1s/step - accuracy: 0.7600 - f1_score: 0.7594 - loss: 0.6700 - precision: 0.8455 - recall: 0.6917 - val_accuracy: 0.9562 - val_f1_score: 0.9624 - val_loss: 0.1366 - val_precision: 0.9690 - val_recall: 0.9441\n",
      "Epoch 3/100\n",
      "166/166 - 243s - 1s/step - accuracy: 0.7792 - f1_score: 0.7776 - loss: 0.5847 - precision: 0.8360 - recall: 0.7236 - val_accuracy: 0.9381 - val_f1_score: 0.9461 - val_loss: 0.1678 - val_precision: 0.9616 - val_recall: 0.9267\n",
      "Epoch 4/100\n",
      "166/166 - 239s - 1s/step - accuracy: 0.8357 - f1_score: 0.8356 - loss: 0.4923 - precision: 0.8733 - recall: 0.7817 - val_accuracy: 0.9471 - val_f1_score: 0.9549 - val_loss: 0.1996 - val_precision: 0.9576 - val_recall: 0.9222\n",
      "Epoch 5/100\n",
      "166/166 - 243s - 1s/step - accuracy: 0.8412 - f1_score: 0.8416 - loss: 0.4388 - precision: 0.8843 - recall: 0.8098 - val_accuracy: 0.9358 - val_f1_score: 0.9514 - val_loss: 0.1616 - val_precision: 0.9582 - val_recall: 0.9169\n",
      "Epoch 6/100\n",
      "166/166 - 241s - 1s/step - accuracy: 0.8407 - f1_score: 0.8408 - loss: 0.4345 - precision: 0.8749 - recall: 0.8183 - val_accuracy: 0.9683 - val_f1_score: 0.9701 - val_loss: 0.1225 - val_precision: 0.9784 - val_recall: 0.9592\n",
      "Epoch 7/100\n",
      "166/166 - 240s - 1s/step - accuracy: 0.8715 - f1_score: 0.8717 - loss: 0.3468 - precision: 0.8975 - recall: 0.8424 - val_accuracy: 0.8731 - val_f1_score: 0.9182 - val_loss: 0.2541 - val_precision: 0.8763 - val_recall: 0.8671\n",
      "Epoch 8/100\n",
      "166/166 - 242s - 1s/step - accuracy: 0.8825 - f1_score: 0.8823 - loss: 0.3125 - precision: 0.8965 - recall: 0.8608 - val_accuracy: 0.9789 - val_f1_score: 0.9799 - val_loss: 0.0920 - val_precision: 0.9824 - val_recall: 0.9683\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.614109Z",
     "start_time": "2026-01-21T21:32:51.609560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#transfer learning. fine tuning\n",
    "\n",
    "if fine_tune_flag:\n",
    "    # inception_fine_tune_model.fit(\n",
    "    #     transfer_model_gen,\n",
    "    #     validation_data = transfer_model_val_gen,\n",
    "    #     epochs=3,\n",
    "    #     class_weight = class_weights_training,\n",
    "    #     callbacks=[StopCallback,ResetValLossOnTrainBegin()],\n",
    "    #     verbose = 2 #2 is one line per epoch -\n",
    "    # )\n",
    "\n",
    "    history_fine_tune = inception_fine_tune_model.fit(\n",
    "        transfer_model_gen,\n",
    "        validation_data = transfer_model_val_gen,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback,ResetValLossOnTrainBegin()],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.643855Z",
     "start_time": "2026-01-21T21:32:51.639108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transfer learning. fine tuning of full model\n",
    "# propably make a scheduler for the learning rate.\n",
    "# Also - train model sucessively\n",
    "\n",
    "if full_fine_tune_flag:\n",
    "    # inception_fine_tune_model.fit(\n",
    "    #     transfer_model_gen,\n",
    "    #     validation_data = transfer_model_val_gen,\n",
    "    #     epochs=3,\n",
    "    #     class_weight = class_weights_training,\n",
    "    #     callbacks=[StopCallback,ResetValLossOnTrainBegin()],\n",
    "    #     verbose = 2 #2 is one line per epoch -\n",
    "    # )\n",
    "\n",
    "    history_full_fine_tune = inception_full_fine_tune_model.fit(\n",
    "        transfer_model_gen,\n",
    "        validation_data = transfer_model_val_gen,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback,ResetValLossOnTrainBegin()],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.656600Z",
     "start_time": "2026-01-21T21:32:51.650860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transfer learning. fine tuning of full model\n",
    "# multi phase fine tuning\n",
    "\n",
    "#delete history if it already exists to unsure expected local stopping behaviour\n",
    "if 'history_multi_fine_tune' in locals():\n",
    "    del history_multi_fine_tune\n",
    "\n",
    "if multi_phase_fine_tune_flag:\n",
    "\n",
    "    match StoppingSelector:\n",
    "        case 'val_loss':\n",
    "            StopCallback_multi = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.01,\n",
    "                patience= loss_stop_patience_multi[0],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                start_from_epoch = 1\n",
    "            )\n",
    "        case 'val_f1':\n",
    "            StopCallback_multi = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_f1_score',\n",
    "                min_delta=0.005,\n",
    "                patience= loss_stop_patience_multi[0],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                mode='max'\n",
    "            )\n",
    "\n",
    "    # train last layer first\n",
    "    history_multi_fine_tune = inception_multiPhase_fine_tune_model.fit(\n",
    "        transfer_model_gen,\n",
    "        validation_data = transfer_model_val_gen,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback_multi],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.668201Z",
     "start_time": "2026-01-21T21:32:51.662604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if multi_phase_fine_tune_flag:\n",
    "    #delete old history to avoid early stopping unexpected behaviour\n",
    "    del history_multi_fine_tune\n",
    "\n",
    "    #set last 30 layers to be trainable\n",
    "    for layer in base_transfer_model_4.layers[-30:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    match StoppingSelector:\n",
    "        case 'val_loss':\n",
    "            StopCallback_multi = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.01,\n",
    "                patience= loss_stop_patience_multi[1],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                start_from_epoch = 1\n",
    "            )\n",
    "        case 'val_f1':\n",
    "            StopCallback_multi = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_f1_score',\n",
    "                min_delta=0.005,\n",
    "                patience= loss_stop_patience_multi[1],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                mode='max'\n",
    "            )\n",
    "\n",
    "    inception_multiPhase_fine_tune_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), #go to lower learning rate\n",
    "        loss=loss_fun_multi_fine,\n",
    "        #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "        #metrics=[\"accuracy\",'precision',]\n",
    "        weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    "    )\n",
    "\n",
    "    history_multi_fine_tune = inception_multiPhase_fine_tune_model.fit(\n",
    "        transfer_model_gen,\n",
    "        validation_data = transfer_model_val_gen,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback_multi,ResetValLossOnTrainBegin()],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.681113Z",
     "start_time": "2026-01-21T21:32:51.675205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if multi_phase_fine_tune_flag:\n",
    "\n",
    "    match StoppingSelector:\n",
    "        case 'val_loss':\n",
    "            StopCallback_multi = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.01,\n",
    "                patience= loss_stop_patience_multi[2],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                start_from_epoch = 1\n",
    "            )\n",
    "        case 'val_f1':\n",
    "            StopCallback_multi = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_f1_score',\n",
    "                min_delta=0.005,\n",
    "                patience= loss_stop_patience_multi[2],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                mode='max'\n",
    "            )\n",
    "\n",
    "    #delete old history to avoid early stopping unexpected behaviour\n",
    "    del history_multi_fine_tune\n",
    "\n",
    "    #set last 100 layers to trainable\n",
    "    for layer in base_transfer_model_4.layers[-100:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    inception_multiPhase_fine_tune_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), #go to lower learning rate\n",
    "        loss=loss_fun_multi_fine,\n",
    "        #loss=tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "        #metrics=[\"accuracy\",'precision',]\n",
    "        weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    "    )\n",
    "\n",
    "    history_multi_fine_tune = inception_multiPhase_fine_tune_model.fit(\n",
    "        transfer_model_gen,\n",
    "        validation_data = transfer_model_val_gen,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback_multi,ResetValLossOnTrainBegin()],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.692532Z",
     "start_time": "2026-01-21T21:32:51.687117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#train fine tuning model with reduced number of label classes\n",
    "\n",
    "if 'history_multi_redLabel' in locals():\n",
    "    del history_multi_redLabel\n",
    "\n",
    "if reducedClassNumber_flag:\n",
    "\n",
    "    match StoppingSelector:\n",
    "        case 'val_loss':\n",
    "            StopCallback_redLabel = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.01,\n",
    "                patience= loss_stop_patience_multi[0],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                start_from_epoch = 1\n",
    "            )\n",
    "        case 'val_f1':\n",
    "            StopCallback_redLabel = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_f1_score',\n",
    "                min_delta=0.005,\n",
    "                patience= loss_stop_patience_multi[0],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                mode='max'\n",
    "            )\n",
    "\n",
    "    # train last layer first\n",
    "    history_multi_redLabel = inception_multiPhase_redLabel.fit(\n",
    "        train_generator_redLabel_aug,\n",
    "        validation_data = train_generator_redLabel_val,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_redLabel_dict,\n",
    "        callbacks=[StopCallback_redLabel],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.706458Z",
     "start_time": "2026-01-21T21:32:51.700537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if reducedClassNumber_flag:\n",
    "\n",
    "    del history_multi_redLabel\n",
    "\n",
    "    match StoppingSelector:\n",
    "        case 'val_loss':\n",
    "            StopCallback_redLabel = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.01,\n",
    "                patience= loss_stop_patience_multi[1],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                start_from_epoch = 1\n",
    "            )\n",
    "        case 'val_f1':\n",
    "            StopCallback_redLabel = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_f1_score',\n",
    "                min_delta=0.005,\n",
    "                patience= loss_stop_patience_multi[1],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                mode='max'\n",
    "            )\n",
    "\n",
    "    #set last 30 layers to be trainable\n",
    "    for layer in base_transfer_model_5.layers[-30:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    inception_multiPhase_redLabel.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), #go to lower learning rate\n",
    "        loss=loss_fun_multi_redLabel,\n",
    "        weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    "    )\n",
    "\n",
    "    history_multi_redLabel = inception_multiPhase_redLabel.fit(\n",
    "        train_generator_redLabel_aug,\n",
    "        validation_data = train_generator_redLabel_val,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_redLabel_dict,\n",
    "        callbacks=[StopCallback_redLabel],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.718305Z",
     "start_time": "2026-01-21T21:32:51.712449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if reducedClassNumber_flag:\n",
    "\n",
    "    #del history_multi_redLabel\n",
    "\n",
    "    match StoppingSelector:\n",
    "        case 'val_loss':\n",
    "            StopCallback_redLabel = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.01,\n",
    "                patience= loss_stop_patience_multi[2],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                start_from_epoch = 1\n",
    "            )\n",
    "        case 'val_f1':\n",
    "            StopCallback_redLabel = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_f1_score',\n",
    "                min_delta=0.005,\n",
    "                patience= loss_stop_patience_multi[2],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                mode='max'\n",
    "            )\n",
    "\n",
    "    #set last 30 layers to be trainable\n",
    "    for layer in base_transfer_model_5.layers[-100:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    inception_multiPhase_redLabel.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), #go to lower learning rate\n",
    "        loss=loss_fun_multi_redLabel,\n",
    "        weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    "    )\n",
    "\n",
    "    history_multi_redLabel = inception_multiPhase_redLabel.fit(\n",
    "        train_generator_redLabel_aug,\n",
    "        validation_data = train_generator_redLabel_val,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_redLabel_dict,\n",
    "        callbacks=[StopCallback_redLabel],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.730749Z",
     "start_time": "2026-01-21T21:32:51.725309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#multi phase fine tuning with efficientnet\n",
    "\n",
    "#delete history if it already exists to unsure expected local stopping behaviour\n",
    "if 'history_EfficientNet' in locals():\n",
    "    del history_EfficientNet\n",
    "\n",
    "if EfficientNet_Flag:\n",
    "\n",
    "    match StoppingSelector:\n",
    "        case 'val_loss':\n",
    "            StopCallback_EfficientNet = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.01,\n",
    "                patience= loss_stop_patience_multi[0],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                start_from_epoch = 1\n",
    "            )\n",
    "        case 'val_f1':\n",
    "            StopCallback_EfficientNet = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_f1_score',\n",
    "                min_delta=0.005,\n",
    "                patience= loss_stop_patience_multi[0],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                mode='max'\n",
    "            )\n",
    "\n",
    "    # train last layer first\n",
    "    history_EfficientNet = EfficientNet_multiPhase_model.fit(\n",
    "        transfer_model_gen,\n",
    "        validation_data = transfer_model_val_gen,\n",
    "        epochs=10,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback_EfficientNet],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:32:51.745424Z",
     "start_time": "2026-01-21T21:32:51.738755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if EfficientNet_Flag:\n",
    "\n",
    "    del history_EfficientNet\n",
    "\n",
    "    match StoppingSelector:\n",
    "        case 'val_loss':\n",
    "            StopCallback_EfficientNet = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.01,\n",
    "                patience= loss_stop_patience_multi[1],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                start_from_epoch = 1\n",
    "            )\n",
    "        case 'val_f1':\n",
    "            StopCallback_EfficientNet = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_f1_score',\n",
    "                min_delta=0.005,\n",
    "                patience= loss_stop_patience_multi[1],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                mode='max'\n",
    "            )\n",
    "\n",
    "    #set last 30 layers to be trainable\n",
    "    for layer in base_model_efficientNet.layers[-30:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    EfficientNet_multiPhase_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), #go to lower learning rate\n",
    "        loss=loss_fun_multi_redLabel,\n",
    "        weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    "    )\n",
    "\n",
    "    history_EfficientNet = EfficientNet_multiPhase_model.fit(\n",
    "        transfer_model_gen,\n",
    "        validation_data = transfer_model_val_gen,\n",
    "        epochs=10,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback_EfficientNet],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if EfficientNet_Flag:\n",
    "\n",
    "    #del history_EfficientNet\n",
    "\n",
    "    match StoppingSelector:\n",
    "        case 'val_loss':\n",
    "            StopCallback_EfficientNet = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0.01,\n",
    "                patience= loss_stop_patience_multi[2],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                start_from_epoch = 1\n",
    "            )\n",
    "        case 'val_f1':\n",
    "            StopCallback_EfficientNet = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_f1_score',\n",
    "                min_delta=0.005,\n",
    "                patience= loss_stop_patience_multi[2],\n",
    "                restore_best_weights=True,\n",
    "                verbose = 2,\n",
    "                mode='max'\n",
    "            )\n",
    "\n",
    "    #set last 30 layers to be trainable\n",
    "    for layer in base_model_efficientNet.layers[-100:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    EfficientNet_multiPhase_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), #go to lower learning rate\n",
    "        loss=loss_fun_multi_redLabel,\n",
    "        weighted_metrics=[\"accuracy\",'precision','recall',tf.keras.metrics.F1Score(average='weighted')]\n",
    "    )\n",
    "\n",
    "    history_EfficientNet = EfficientNet_multiPhase_model.fit(\n",
    "        transfer_model_gen,\n",
    "        validation_data = transfer_model_val_gen,\n",
    "        epochs=max_epochs,\n",
    "        class_weight = class_weights_training,\n",
    "        callbacks=[StopCallback_EfficientNet],\n",
    "        verbose = 2 #2 is one line per epoch -\n",
    "    )"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "[Clearly specify which metrics you'll use to evaluate the model performance, and why you've chosen these metrics.]\n",
    "\n",
    "Like stated on the .md files in the project we mainly focus on the F1 Score. However, for each model and hyperparameter set a classification report including class-specific metrics is save to a .csv file for immidiate and later evaluation\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model 2\n",
    "\n",
    "if model2_flag:\n",
    "\n",
    "    #make one folder for each model to save metrics\n",
    "    model_2_dir = os.path.join(hyperparam_dir,'model_2')\n",
    "    if not os.path.isdir(model_2_dir):\n",
    "        os.makedirs(model_2_dir)\n",
    "\n",
    "    # test accuracy on test data\n",
    "    if balanced_flag:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_2_CNN.evaluate(test_generator)\n",
    "\n",
    "        #for classification report\n",
    "        true_labels = test_generator_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = model_2_CNN.predict(test_generator_metrics)\n",
    "\n",
    "    else:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_2_CNN.evaluate(test_generator_unbalanced)\n",
    "\n",
    "        true_labels = test_generator_unbalanced_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = model_2_CNN.predict(test_generator_unbalanced_metrics)\n",
    "\n",
    "    #convert to numerical - np.argmax directly does the job\n",
    "    predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "    print(f\"Model 2. Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "    print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "    #save as dict for future use as well\n",
    "    report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "    #convert to dataframe for easy use and saving to csv\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    #save to file\n",
    "    metrics_baseline_savename = os.path.join(model_2_dir,'classification_report.csv')\n",
    "\n",
    "    report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "    #save model as well for future use\n",
    "    #save the model:\n",
    "    model_2_CNN.save(os.path.join(model_2_dir,'model.keras'))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model 3\n",
    "\n",
    "if model3_flag:\n",
    "\n",
    "    #make one folder for each model to save metrics\n",
    "    model_3_dir = os.path.join(hyperparam_dir,'model_3')\n",
    "    if not os.path.isdir(model_3_dir):\n",
    "        os.makedirs(model_3_dir)\n",
    "\n",
    "    # test accuracy on test data\n",
    "    if balanced_flag:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_3_CNN.evaluate(test_generator)\n",
    "\n",
    "        #for classification report\n",
    "        true_labels = test_generator_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = model_3_CNN.predict(test_generator_metrics)\n",
    "\n",
    "    else:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_3_CNN.evaluate(test_generator_unbalanced)\n",
    "\n",
    "        true_labels = test_generator_unbalanced_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = model_3_CNN.predict(test_generator_unbalanced_metrics)\n",
    "\n",
    "    #convert to numerical - np.argmax directly does the job\n",
    "    predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "    print(f\"Model 3. Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "    print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "    #save as dict for future use as well\n",
    "    report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "    #convert to dataframe for easy use and saving to csv\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    #save to file\n",
    "    metrics_baseline_savename = os.path.join(model_3_dir,'classification_report.csv')\n",
    "\n",
    "    report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "    #save model as well for future use\n",
    "    #save the model:\n",
    "    model_3_CNN.save(os.path.join(model_3_dir,'model.keras'))"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:34:48.210744Z",
     "start_time": "2026-01-21T21:34:00.976172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model 1\n",
    "\n",
    "if model1_flag:\n",
    "\n",
    "    #make one folder for each model to save metrics\n",
    "    model_1_dir = os.path.join(hyperparam_dir,'model_1')\n",
    "    if not os.path.isdir(model_1_dir):\n",
    "        os.makedirs(model_1_dir)\n",
    "\n",
    "    # test accuracy on test data\n",
    "    if balanced_flag:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_1_CNN.evaluate(test_generator)\n",
    "\n",
    "        #for classification report\n",
    "        true_labels = test_generator_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = model_1_CNN.predict(test_generator_metrics)\n",
    "\n",
    "    else:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = model_1_CNN.evaluate(test_generator_unbalanced)\n",
    "\n",
    "        true_labels = test_generator_unbalanced_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = model_1_CNN.predict(test_generator_unbalanced_metrics)\n",
    "\n",
    "    #convert to numerical - np.argmax directly does the job\n",
    "    predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "    print(f\"Model 1. Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "    print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "    #save as dict for future use as well\n",
    "    report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "    #convert to dataframe for easy use and saving to csv\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    #save to file\n",
    "    metrics_baseline_savename = os.path.join(model_1_dir,'classification_report.csv')\n",
    "\n",
    "    report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "    #save model as well for future use\n",
    "    #save the model:\n",
    "    model_1_CNN.save(os.path.join(model_1_dir,'model.keras'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m52/52\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 213ms/step - accuracy: 0.9825 - f1_score: 0.9823 - loss: 0.1455 - precision: 0.9825 - recall: 0.9819\n",
      "\u001B[1m1656/1656\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 19ms/step\n",
      "Model 1. Test Accuracy: 0.982 | Test Loss: 0.145 | Test Precision: 0.982 | Test Recall: 0.982 | Test F1 Score: 0.982:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "             0_GOOD       0.97      0.99      0.98       238\n",
      "        1_Flat loop       0.64      0.60      0.62        15\n",
      "   2_White lift-off       0.85      0.91      0.88        57\n",
      "   3_Black lift-off       0.83      0.77      0.80        13\n",
      "          4_Missing       1.00      0.99      1.00      1316\n",
      "5_Short circuit MOS       0.86      0.71      0.77        17\n",
      "\n",
      "           accuracy                           0.98      1656\n",
      "          macro avg       0.86      0.83      0.84      1656\n",
      "       weighted avg       0.98      0.98      0.98      1656\n",
      "\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:37:22.046907Z",
     "start_time": "2026-01-21T21:34:48.215748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model transfer feature extraction\n",
    "\n",
    "if feature_extract_flag:\n",
    "\n",
    "    #make one folder for each model to save metrics\n",
    "    model_feat_extract_dir = os.path.join(hyperparam_dir,'InceptionV3_feat_extract')\n",
    "    if not os.path.isdir(model_feat_extract_dir):\n",
    "        os.makedirs(model_feat_extract_dir)\n",
    "\n",
    "    # test accuracy on test data\n",
    "    if balanced_flag:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_feature_extraction_model.evaluate(test_generator_color)\n",
    "\n",
    "        #for classification report\n",
    "        true_labels = test_generator_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = inception_feature_extraction_model.predict(test_generator_metrics_color)\n",
    "\n",
    "    else:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_feature_extraction_model.evaluate(test_generator_unbalanced_color)\n",
    "\n",
    "        true_labels = test_generator_unbalanced_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = inception_feature_extraction_model.predict(test_generator_unbalanced_metrics_color)\n",
    "\n",
    "    #convert to numerical - np.argmax directly does the job\n",
    "    predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "    print(f\"Feat. Extract. Model:  Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "    print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "    #save as dict for future use as well\n",
    "    report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "    #convert to dataframe for easy use and saving to csv\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    #save to file\n",
    "    metrics_baseline_savename = os.path.join(model_feat_extract_dir,'classification_report.csv')\n",
    "\n",
    "    report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "    #save model as well for future use\n",
    "    #save the model:\n",
    "    inception_feature_extraction_model.save(os.path.join(model_feat_extract_dir,'model.keras'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m52/52\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m58s\u001B[0m 1s/step - accuracy: 0.9577 - f1_score: 0.9637 - loss: 0.1293 - precision: 0.9734 - recall: 0.9517\n",
      "\u001B[1m1656/1656\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m94s\u001B[0m 56ms/step\n",
      "Feat. Extract. Model:  Test Accuracy: 0.958 | Test Loss: 0.129 | Test Precision: 0.973 | Test Recall: 0.952 | Test F1 Score: 0.964:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "             0_GOOD       0.94      0.85      0.89       238\n",
      "        1_Flat loop       0.56      0.60      0.58        15\n",
      "   2_White lift-off       0.87      0.79      0.83        57\n",
      "   3_Black lift-off       0.22      0.85      0.35        13\n",
      "          4_Missing       1.00      0.99      1.00      1316\n",
      "5_Short circuit MOS       0.73      0.65      0.69        17\n",
      "\n",
      "           accuracy                           0.96      1656\n",
      "          macro avg       0.72      0.79      0.72      1656\n",
      "       weighted avg       0.97      0.96      0.96      1656\n",
      "\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:37:22.074988Z",
     "start_time": "2026-01-21T21:37:22.067913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#for tine tuning model\n",
    "\n",
    "if fine_tune_flag:\n",
    "\n",
    "    #make one folder for each model to save metrics\n",
    "    model_feat_extract_dir = os.path.join(hyperparam_dir,'InceptionV3_fine_tune')\n",
    "    if not os.path.isdir(model_feat_extract_dir):\n",
    "        os.makedirs(model_feat_extract_dir)\n",
    "\n",
    "    # test accuracy on test data\n",
    "    if balanced_flag:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_fine_tune_model.evaluate(test_generator_color)\n",
    "\n",
    "        #for classification report\n",
    "        true_labels = test_generator_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = inception_fine_tune_model.predict(test_generator_metrics_color)\n",
    "\n",
    "    else:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_fine_tune_model.evaluate(test_generator_unbalanced_color)\n",
    "\n",
    "        true_labels = test_generator_unbalanced_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = inception_fine_tune_model.predict(test_generator_unbalanced_metrics_color)\n",
    "\n",
    "    #convert to numerical - np.argmax directly does the job\n",
    "    predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "    print(f\"Fine tune Model:  Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "    print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "    #save as dict for future use as well\n",
    "    report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "    #convert to dataframe for easy use and saving to csv\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    #save to file\n",
    "    metrics_baseline_savename = os.path.join(model_feat_extract_dir,'classification_report.csv')\n",
    "\n",
    "    report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "    #save model as well for future use\n",
    "    #save the model:\n",
    "    inception_fine_tune_model.save(os.path.join(model_feat_extract_dir,'model.keras'))"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:37:22.092894Z",
     "start_time": "2026-01-21T21:37:22.086993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#for full tine tuning model\n",
    "\n",
    "if full_fine_tune_flag:\n",
    "\n",
    "    #make one folder for each model to save metrics\n",
    "    model_full_fine_tune_dir = os.path.join(hyperparam_dir,'InceptionV3_full_fine_tune')\n",
    "    if not os.path.isdir(model_full_fine_tune_dir):\n",
    "        os.makedirs(model_full_fine_tune_dir)\n",
    "\n",
    "    # test accuracy on test data\n",
    "    if balanced_flag:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_full_fine_tune_model.evaluate(test_generator_color)\n",
    "\n",
    "        #for classification report\n",
    "        true_labels = test_generator_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = inception_full_fine_tune_model.predict(test_generator_metrics_color)\n",
    "\n",
    "    else:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_full_fine_tune_model.evaluate(test_generator_unbalanced_color)\n",
    "\n",
    "        true_labels = test_generator_unbalanced_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = inception_full_fine_tune_model.predict(test_generator_unbalanced_metrics_color)\n",
    "\n",
    "    #convert to numerical - np.argmax directly does the job\n",
    "    predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "    print(f\"Full fine tune Model:  Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "    print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "    #save as dict for future use as well\n",
    "    report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "    #convert to dataframe for easy use and saving to csv\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    #save to file\n",
    "    metrics_baseline_savename = os.path.join(model_full_fine_tune_dir,'classification_report.csv')\n",
    "\n",
    "    report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "    #save model as well for future use\n",
    "    #save the model:\n",
    "    inception_full_fine_tune_model.save(os.path.join(model_full_fine_tune_dir,'model.keras'))"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:37:22.103554Z",
     "start_time": "2026-01-21T21:37:22.097889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#for multi phase full tine tuning model - reduced class number\n",
    "if reducedClassNumber_flag:\n",
    "\n",
    "    #make one folder for each model to save metrics\n",
    "    model_fine_tune_redLabel_dir = os.path.join(hyperparam_dir,'InceptionV3_MultiPhase_reducedLabelNum')\n",
    "    if not os.path.isdir(model_fine_tune_redLabel_dir):\n",
    "        os.makedirs(model_fine_tune_redLabel_dir)\n",
    "\n",
    "    # test accuracy on test data - test accuracy ALWAYS on full test set\n",
    "\n",
    "    test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_multiPhase_redLabel.evaluate(test_generator_redLabel)\n",
    "\n",
    "    #for classification report\n",
    "    true_labels = test_generator_redLabel_metrics.classes\n",
    "    # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "    predicted_labels = inception_multiPhase_redLabel.predict(test_generator_redLabel_metrics)\n",
    "\n",
    "    #convert to numerical - np.argmax directly does the job\n",
    "    predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "    print(f\"Reduced Label Number:  Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "    print(classification_report(true_labels, predicted_labels,target_names = label_list_reduced))\n",
    "\n",
    "    #save as dict for future use as well\n",
    "    report = classification_report(true_labels, predicted_labels,target_names = label_list_reduced,output_dict=True)\n",
    "    #convert to dataframe for easy use and saving to csv\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    #save to file\n",
    "    metrics_baseline_savename = os.path.join(model_fine_tune_redLabel_dir,'classification_report.csv')\n",
    "\n",
    "    report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "    #save model as well for future use\n",
    "    #save the model:\n",
    "    inception_multiPhase_redLabel.save(os.path.join(model_fine_tune_redLabel_dir,'model.keras'))"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:37:22.115743Z",
     "start_time": "2026-01-21T21:37:22.109557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#for multi phase full tine tuning model\n",
    "\n",
    "if multi_phase_fine_tune_flag:\n",
    "\n",
    "    #make one folder for each model to save metrics\n",
    "    model_multi_full_fine_tune_dir = os.path.join(hyperparam_dir,'InceptionV3_MultiPhase_full_fine_tune')\n",
    "    if not os.path.isdir(model_multi_full_fine_tune_dir):\n",
    "        os.makedirs(model_multi_full_fine_tune_dir)\n",
    "\n",
    "    # test accuracy on test data - test accuracy ALWAYS on full test set\n",
    "    if balanced_flag:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_multiPhase_fine_tune_model.evaluate(test_generator_color)\n",
    "\n",
    "        #for classification report\n",
    "        true_labels = test_generator_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = inception_multiPhase_fine_tune_model.predict(test_generator_metrics_color)\n",
    "\n",
    "    else:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = inception_multiPhase_fine_tune_model.evaluate(test_generator_unbalanced_color)\n",
    "\n",
    "        true_labels = test_generator_unbalanced_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = inception_multiPhase_fine_tune_model.predict(test_generator_unbalanced_metrics_color)\n",
    "\n",
    "    #convert to numerical - np.argmax directly does the job\n",
    "    predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "    print(f\"Full fine tune Model:  Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "    print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "    #save as dict for future use as well\n",
    "    report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "    #convert to dataframe for easy use and saving to csv\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    #save to file\n",
    "    metrics_baseline_savename = os.path.join(model_multi_full_fine_tune_dir,'classification_report.csv')\n",
    "\n",
    "    report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "    #save model as well for future use\n",
    "    #save the model:\n",
    "    inception_multiPhase_fine_tune_model.save(os.path.join(model_multi_full_fine_tune_dir,'model.keras'))"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:37:22.127488Z",
     "start_time": "2026-01-21T21:37:22.121746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if EfficientNet_Flag:\n",
    "\n",
    "    #make one folder for each model to save metrics\n",
    "    model_EfficientNet_fine_tune_dir = os.path.join(hyperparam_dir,'EfficientNet_MultiPhase_fine_tune')\n",
    "    if not os.path.isdir(model_EfficientNet_fine_tune_dir):\n",
    "        os.makedirs(model_EfficientNet_fine_tune_dir)\n",
    "\n",
    "    # test accuracy on test data - test accuracy ALWAYS on full test set\n",
    "    if balanced_flag:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = EfficientNet_multiPhase_model.evaluate(test_generator_color)\n",
    "\n",
    "        #for classification report\n",
    "        true_labels = test_generator_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = inception_multiPhase_fine_tune_model.predict(test_generator_metrics_color)\n",
    "\n",
    "    else:\n",
    "        test_loss, test_accuracy, test_precision, test_recall,test_f1_score = EfficientNet_multiPhase_model.evaluate(test_generator_unbalanced_color)\n",
    "\n",
    "        true_labels = test_generator_unbalanced_metrics.classes\n",
    "        # model.predict directly gives you the output of the last mode layer. so percentages when using i.e. 'softmax'\n",
    "        predicted_labels = EfficientNet_multiPhase_model.predict(test_generator_unbalanced_metrics_color)\n",
    "\n",
    "    #convert to numerical - np.argmax directly does the job\n",
    "    predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "    print(f\"Full fine tune Model:  Test Accuracy: {test_accuracy:.3g} | Test Loss: {test_loss:.3g} | Test Precision: {test_precision:.3g} | Test Recall: {test_recall:.3g} | Test F1 Score: {test_f1_score:.3g}:\")\n",
    "\n",
    "    print(classification_report(true_labels, predicted_labels,target_names = label_list))\n",
    "\n",
    "    #save as dict for future use as well\n",
    "    report = classification_report(true_labels, predicted_labels,target_names = label_list,output_dict=True)\n",
    "    #convert to dataframe for easy use and saving to csv\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    #save to file\n",
    "    metrics_baseline_savename = os.path.join(model_EfficientNet_fine_tune_dir,'classification_report.csv')\n",
    "\n",
    "    report_df.to_csv(metrics_baseline_savename)\n",
    "\n",
    "    #save model as well for future use\n",
    "    #save the model:\n",
    "    EfficientNet_multiPhase_model.save(os.path.join(model_EfficientNet_fine_tune_dir,'model.keras'))"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:37:22.144712Z",
     "start_time": "2026-01-21T21:37:22.141493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#clear all models from memory to prevent any bugs and weird behaviour of early stopping\n",
    "\n",
    "#see: https://stackoverflow.com/questions/58137677/keras-model-training-memory-leak\n",
    "\n",
    "del model_1_CNN\n",
    "del model_2_CNN\n",
    "del model_3_CNN\n",
    "\n",
    "del inception_feature_extraction_model\n",
    "del base_transfer_model\n",
    "del inception_fine_tune_model\n",
    "del base_transfer_model_2\n",
    "del inception_full_fine_tune_model\n",
    "del base_transfer_model_3\n",
    "del inception_multiPhase_fine_tune_model\n",
    "del base_transfer_model_4"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:37:24.035682Z",
     "start_time": "2026-01-21T21:37:22.166718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session(\n",
    "    free_memory=True\n",
    ")\n",
    "tf.compat.v1.reset_default_graph()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nikoLocal\\PycharmProjects\\MV_Valeo_Opencampus\\.venv\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "[Compare the performance of your model(s) against the baseline model. Discuss any improvements or setbacks and the reasons behind them.]\n",
    "\n",
    "A table comparing the performances of different models and hyperparameter settings can be found in the github (Model_Performance_overview.xls or Model_Performance_overview.csv).\n",
    "\n",
    "Some results stand out:\n",
    "\n",
    "* data augmentation seems to lower model performance across the board even when we see overfitting in training. The likely reason is that the data itself is very regular without a lot of orientation of the features in the images. Therefore, moderate data augmentation was used only. However, there is very likely still a lot of room for improvement here.\n",
    "* The transfer learning model performs worse than the 3 relatively simple models. Especially for low image resolutions. The most likely reason is that, as of now we only use feature extraction. For any image size that the model was not originally trained on this will very likely mean a bad performance. For higher resolutions the transfer learning model performs better in comparison\n",
    "* Higher image resolution does not really improve model performance.\n",
    "\n",
    "Some things are still missing in the analysis / evaluation and will be added in the near future:\n",
    "\n",
    "* Transfer learning models with fine tuning\n",
    "* Different transfer learning base architectures\n",
    "* When a best model is found we will tackle the task of identifying the drift label class\n",
    "* More finetuning of hyperparameters for few selected models\n",
    "* class weighting instead of balanced dataset (balanced dataset is very small)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Update on model performance after the presentation\n",
    "\n",
    "* Transfer learning Inception V3: using multi-phase fine tuning drastically improved performance\n",
    "* Best performing model still depends on hyperparameters -> there is very likely a lot of room for improvement in systematic hyperparameter variation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
